(window.webpackJsonp=window.webpackJsonp||[]).push([[276],{644:function(s,a,t){"use strict";t.r(a);var e=t(5),n=Object(e.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h2",{attrs:{id:"第一到3章在一个文档里没有补充"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#第一到3章在一个文档里没有补充"}},[s._v("#")]),s._v(" 第一到3章在一个文档里没有补充")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("U2FsdGVkX1+/pn+sQuF72i7CsWJ+9uHIoaFL6U7K2/rB5ctmOD3ALZMZn9ztEMZA\nSbdu0lhFdPAzs7dPb80d/lyw7QzZplwD/hFirzKipkJQfyOwmLNbKblgRZOb00jt\n/g==\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("blockquote",[a("p",[s._v("上节课内容回顾")])]),s._v(" "),a("p",[s._v("这一章我们就会进入非常有趣的部分，真正开始进行机器学习。课程开始要给大家说明下，机器学习这个领域，更多的是使用python语言或者R语言，而spark的java代码，反而会显得相对比较重。所以，在介绍机器学习时，会同时引入python和spark的代码给大家做演示，目的是为了让大家通过对比，也增加一点对于python尤其是sklearn框架的了解，这样，基于我们的课程，大家就可以去kaggle上转一转了。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("U2FsdGVkX18mrtT2ZCpsiuEQ3fXY0UnoIFHgNd4M3NtwQWcUD871yuB74HDEQnBB\nVhj/WiGNtsgJbvR0P+zcYB5HTPoy3Np5NEjPFJGZWl1q/jX0fg==\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h2",{attrs:{id:"四、机器学习-特征工程基础"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#四、机器学习-特征工程基础"}},[s._v("#")]),s._v(" 四、机器学习-特征工程基础")]),s._v(" "),a("h3",{attrs:{id:"_1、机器学习处理什么问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、机器学习处理什么问题"}},[s._v("#")]),s._v(" 1、机器学习处理什么问题")]),s._v(" "),a("p",[s._v("机器学习处理的问题通常都是一些规律不是特别明显的问题，这类问题通常很难通过简单的抽取规律来解决。举个例子，在公务员考试中，通常都会有这样的逻辑题：给你一串数字， 1,3,5,7,?  然后让你去推断下一个数字。或许这个题太简单，你知道要填个9，这就是一串奇数嘛。这只是一个简单的计算过程，换一种说法，这也就是一个预测的过程。通过已有的数据寻找规律，预测出下一个数字是什么样的。")]),s._v(" "),a("blockquote",[a("p",[s._v("有个小问题 如果有这样一串数字 1,3,4,7,9,11,?  你觉得这个?是什么数字？")])]),s._v(" "),a("p",[s._v("在机器学习中，最为典型的分类算法和回归算法，他们的处理流程也是类似于这样一个过程。通过对历史数据的推断，寻找数字之间的规律，从而预测出后面的数字是什么样的。只不过，他要处理的问题，比这个简单的数字推断更复杂。机器学习的历史数据不再是一个一个的数字，而是由多个数字组成的向量。并且，数据之间的规律更难找到，同时也没有这么稳固。很可能数字之间并没有完全准确的规则，这时就需要选择出一个相对靠谱的数字来。其实这个思想跟之前的数字推断是一样的。")]),s._v(" "),a("blockquote",[a("p",[s._v("机器学习处理的是数字向量的问题，如果特征值是图片要怎么处理？")])]),s._v(" "),a("h3",{attrs:{id:"_2、机器学习的标准处理流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、机器学习的标准处理流程"}},[s._v("#")]),s._v(" 2、机器学习的标准处理流程")]),s._v(" "),a("p",[s._v("​\t一个典型的机器学习项目通常分为以下几个步骤：")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("1- 数据收集")])]),s._v(" "),a("p",[s._v('首先需要收集到尽量多，尽量全的业务数据，这样整个机器学习的结果才更准确，更有说服力。这就好比我们经常说的一句话："所有人都是这么说的"。还是以我们那个逻辑题为例，如果只给你 1,3,?  你或许也能推测出下一个数字是5，但是这个规律就非常勉强了。而反过来，如果这一串数字给得越多，这个规律就会得到不断的验证，最终效果就会越明显，也越能处理实际的问题。例如，1,3,4,7,9,?  如果是这样的题目，本身的规律就变得非常模糊了。但是如果这串数字还按照奇数的规律不断重复，100个数字，10000个数字，这个时候，整个奇数串的规律是不是还是可以接受了？偶尔一两个数字的偏差也是在可接受范围内。其实我们处理实际问题也是这样一个过程，比如预测天气，我们通常会总结出很多的生活规律，比如天气特别闷热，晚上出现毛月亮，那我们就会推测明天会下雨。尽管这个推测很多时候是不靠谱的，但是还是会形成这样的整体规律。为什么呢？这就是因为有别人非常多的经验，这些经验就是来自于对历史数据的学习。')]),s._v(" "),a("p",[s._v("这也符合机器学习的工程要求。机器学习会通过学习历史数据，总结出一些最有可能的规律。当这些规律达到一个比较高的可信度时，就可以用来对未来数据进行预测了。所以数据的体量以及质量，往往就决定了机器学习所能达到的高度。这也是为什么很多好的机器学习产品最先都是出在像谷歌、百度这样的大公司的，就是因为他们的数据往往是最大最全的。")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("2- 数据清洗")])]),s._v(" "),a("p",[s._v("然后有了数据之后还需要进行清洗。原始的数据就像是矿石，往往含金量非常低。这个时候就要通过数据清洗，将明显无用的信息去掉，并且把数据整理成能够被机器学习接收的数据格式。这个过程通常没有固定的工作方式，需要根据不同的算法不同的要求，指定不同的处理方式。数据清洗是前期工作量非常大的一个环节，同时也是非常考验程序员工程能力的环节。")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("2- 训练模型")])]),s._v(" "),a("p",[s._v("这个过程是最关键的，但是其实他也是比较简单的。有了数据之后，你只需要选择一个合适的机器学习算法，把数据交给他学习，自然就会形成一个数据模型。这个过程往往不需要人工进行干预。甚至很多时候，机器学习到底学习到了哪些规律，人也是很难弄明白的。在这个过程中，需要注意的是，针对同一个问题，往往可以选择很多的算法，甚至针对同一个算法，也会需要制定不同的超参数。这些组合都会计算出不同的数据模型。所有这些模型都是可选的结果。")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("3- 模型优化")])]),s._v(" "),a("p",[s._v("​\t训练出了模型，并不能代表机器学习成功。有了众多的数据模型之后，就需要在这些模型中找出针对当前问题的最佳方案。这个优化过程即需要基于对算法的深入了解，同时也需要基于大量的尝试。也是非常考验算法工程师技术的地方。")]),s._v(" "),a("p",[s._v("最常用的检测方案是将整个数据集随机拆分成训练集和测试集。用机器学习算法在训练集上学习并形成数据模型，然后拿这个数据模型对测试集的数据进行预测，接下来拿预测出来的目标值与测试集上实际的目标值进行比对。目标值真实结果匹配度最高的模型就认为是最好的模型。我们经常说的人脸识别准确率达到多少多少，其实就是在测试集上的比对结果。")]),s._v(" "),a("p",[s._v("​\t另外，从历史数据中学习形成的数据模型，最终还是要回归于对未来业务的指导，而未来业务又会形成新的数据集。这个时候模型优化一个很重要的过程就是需要让模型继续学习更多新的业务数据，及时优化。这样才能让模型的准确地更高。")]),s._v(" "),a("h3",{attrs:{id:"_3、为什么需要特征工程-feature-engineering"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、为什么需要特征工程-feature-engineering"}},[s._v("#")]),s._v(" 3、为什么需要特征工程(Feature Engineering)？")]),s._v(" "),a("p",[s._v("​\t我们首先看一个kaggle上的竞赛排名结果  https://www.kaggle.com/c/tabular-playground-series-apr-2021/leaderboard")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175439681.png",alt:"image-20220624175439681"}})]),s._v(" "),a("blockquote",[a("p",[s._v("这个榜单上有很多中国的学生")])]),s._v(" "),a("p",[s._v("可以看到，针对同一个问题，同样的数据集，竞赛中却跑出来了非常多不同的成绩。那大家想一下，数据集是一样的，算法也就那么几种，这些顶级竞赛者肯定也都懂。那他们应该要跑出同样的结果才对。那为什么还能出现这么多结果，排出个名次来呢？这个区别就来自于我们这一章节要讨论的特征工程。")]),s._v(" "),a("p",[s._v("​\t业界流传一句非常经典的话："),a("strong",[s._v("数据和特征决定了机器学习的上限，而模型和算法只是不断逼近这个上限而已")]),s._v("。那要怎么才能逼近呢？这就要靠特征工程。")]),s._v(" "),a("p",[s._v("​\t特征工程是使用专业背景知识和技巧，处理数据，使得特征能够在机器学习算法上发挥更好的作用的过程。也就是说，"),a("strong",[s._v("特征工程会直接影响机器学习的效率")]),s._v("。而这些各种机器学习竞赛，之所以分出不同的高下，很大一部分原因就在于他们的特征工程处理方式不同。")]),s._v(" "),a("p",[s._v("这里可以看到，其实特征工程也就是在数据收集清洗完成之后，在机器学习之前，对数据进行预处理的过程。这个过程，跟机器学习是没有太多关系的。也就是说，做特征工程，其实不需要机器学习的知识，你大可以随心所欲，想怎么做就怎么做。但是，如果你想要让后面机器学习的效果比较好，那有些普遍的工具和处理方法还是要懂的。")]),s._v(" "),a("blockquote",[a("p",[s._v("具体可以参见准备的python sklearn示例，以及spark的官方示例。")])]),s._v(" "),a("h3",{attrs:{id:"_4、常用的特征工程方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、常用的特征工程方法"}},[s._v("#")]),s._v(" 4、常用的特征工程方法")]),s._v(" "),a("p",[s._v("​\t特征工程的方法很多，大致可以按照作用分为以下几种方式：")]),s._v(" "),a("h4",{attrs:{id:"_1、特征抽取"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、特征抽取"}},[s._v("#")]),s._v(" "),a("strong",[s._v("1、特征抽取")])]),s._v(" "),a("p",[s._v("​\t机器学习只能学习数字类型的特征值，但是有些数据集他的原始数据不是数字类型，比如图像、文本、字符等。这时，就需要使用特征抽取将数据转化成适合机器学习的数字特征。")]),s._v(" "),a("p",[a("strong",[s._v("1> 文本特征抽取：")])]),s._v(" "),a("p",[s._v("​\t场景：现在如果我们要对文本的分类情况进行机器学习，这是一个典型的分类问题，但是这个特征值似乎跟我们之前提到的特征值不太一样。文本没有那些属性和数字啊。那应该怎么抽取特征值呢？")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("1、one-hot编码")])]),s._v(" "),a("p",[s._v("对于字典类型的数据特征，例如 性别、城市、颜色这一类字典类型数据，通常在数据库中，会以一个数字编码标识，如 0:男,1:女  这样。但是，如果在机器学习中使用这样的数字编码，就会给学习过程造成误解。因为不同的字典值特性应该是完全“平等”，而如果是  0，1，2这样的数字，则可能给机器学习造成误解，觉得这些字典值是有大小关系的。所以，机器学习中常用的方式是把字典值处理成one-hot编码。")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("性别")]),s._v(" "),a("th",[s._v("性别==男")]),s._v(" "),a("th",[s._v("性别==女")]),s._v(" "),a("th",[s._v("one-hot编码")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("男")]),s._v(" "),a("td",[s._v("1")]),s._v(" "),a("td",[s._v("0")]),s._v(" "),a("td",[s._v("(1,0)")])]),s._v(" "),a("tr",[a("td",[s._v("女")]),s._v(" "),a("td",[s._v("0")]),s._v(" "),a("td",[s._v("1")]),s._v(" "),a("td",[s._v("(0,1)")])])])]),s._v(" "),a("blockquote",[a("p",[s._v("sklearn方法：sklearn.feature_extraction.DictVectorizer")]),s._v(" "),a("p",[s._v("spark示例：OneHotDemo")]),s._v(" "),a("p",[s._v("实现效果是不一样的。")])]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("2、CountVectorize")])]),s._v(" "),a("p",[s._v("对于文本类型的数据，如一篇文章。在做机器学习时，最基础的处理方式是以文章中的单词出现次数作为特征。处理成 [(word1,count1),(word2,count2)…]这样的格式。这也是mapreduce、spark最经典的入门计算方式。")]),s._v(" "),a("blockquote",[a("p",[s._v("sklearn方法：sklearn.feature_extraction.CountVectorizer")]),s._v(" "),a("p",[s._v("spark示例：CountVecDemo")])]),s._v(" "),a("p",[s._v("​\t缺点：在文章分类等机器学习场景中，体现不出文章的重要特征。例如一般出现次数最多的一些词，如，这里、那里、我们、他们等，并不能体现文章的内容特征。这类词称为停用词。")]),s._v(" "),a("p",[s._v("​\t补充：\n在对文本进行特征处理时，一般分词是个绕不开的坎。英文分词比较简单，按空格就行。而中文则麻烦得多。同时中文分词也是一个深度学习的重要研究领域，有一些可用的开源中文分词工具，例如java的hanlp，python的jieba等。使用开源的中文分词工具，很多的词语都要做特殊处理。例如如果你尝试去对鹿鼎记进行拆分，里面的很多名字、武功等就要做特殊处理，例如西奥图三世、白衣尼等。而对于一些更为专业的文章，这些开源的中文分词工具就都表现不是很好了。像百度的中文分词工具， https://cloud.baidu.com/product/nlp/lexical  大家可以上去体验一下。")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("3、TfidfVectorizer")])]),s._v(" "),a("p",[s._v("​\t我们的目的是为了对文本进行分类，但是简单的以单词出现的次数来分类，从经验上判断，会有些问题。长的文章中各个词出现的次数都会比较多，而短的文章各个词普遍都会比较少。这样长的文章对分类结果的影响就会被放大。这时改进的办法就是TF-IDF")]),s._v(" "),a("p",[s._v("TF-IDF可以用来评估一个字词对于一个文件集合或者一个语料库中的其中一份文件的重要程度。例如，在对一大堆文章进行分类时，出现  计算机、软件、云、java这些词的次数比较多的文章更多可能归为科技类(在其他类中出现就比较少，这样的词才有重要性)，而出现 银行、信贷、信用卡  这类词出现次数较多的文章更多可能归为金融类。而所有文章中出现次数都比较多的 我们、你们、这里、那里等这一类的词则对分类来说，意义不大。")]),s._v(" "),a("blockquote",[a("p",[s._v("· TF-IDF由TF和IDF两部分组成，")]),s._v(" "),a("ul",[a("li",[s._v("TF：词频 term frequency。某一个给定的词语在文章中出现的评率")]),s._v(" "),a("li",[s._v("IDF：逆向文档评率 inverse document frequency.是一个词语普遍重要性的度量。 为总文件数目 除以 包含该词语的文件的数量，再取 10为底对数。")])]),s._v(" "),a("p",[s._v("最终 TF-IDF=TF*IDF")]),s._v(" "),a("p",[s._v("示例：\n关键词：“经济”；语料库： 1000篇文章；10篇文章出现“经济”。")]),s._v(" "),a("p",[s._v("TF(经济) = 10/1000 = 0.01 ；IDF(经济)=lg(1000/10)=2")]),s._v(" "),a("p",[s._v("最终 TF-IDF(经济) = TF(经济)*IDF(经济) = 0.02")])]),s._v(" "),a("blockquote",[a("p",[s._v("sklearn方法：sklearn.feature_extraction.CountVectorizer.TfidfVectorizer")]),s._v(" "),a("p",[s._v("spark示例：JavaTfIdfExample")])]),s._v(" "),a("p",[a("strong",[s._v("2> 图像类型怎么进行特征提取？")])]),s._v(" "),a("p",[s._v("​\t图像类型的特征，最简单的特征提取方式，是按照图像每个像素点的RGB三信道值，抽取出三个信道矩阵，这样就有了基础的数学特征。但是图像特征的处理难度太大，机器学习基本处理不了。大都需要基于深度学习，神经网络来做。有兴趣的可以自己去了解下。")]),s._v(" "),a("h4",{attrs:{id:"_2、特征预处理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、特征预处理"}},[s._v("#")]),s._v(" "),a("strong",[s._v("2、特征预处理")])]),s._v(" "),a("p",[s._v("​\t现在我们考虑这样一组特征值： 用户年龄和用户收入。我们能发现，年龄的数字相比收入的数字会小很多。根据之前特征要平等的原则，直觉上就会觉得，这一组数据集中，用户收入的特征会被放大，而用户年龄的特征就容易被忽略。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175452260.png",alt:"image-20220624175452260"}})]),s._v(" "),a("p",[s._v("​\t所以我们在用这样的数据集之前，要把各个特征值的范围尽量统一。这种方式称之为"),a("strong",[s._v("无量纲化")]),s._v("，也就是要在保留数据之间的特征关系的同时，消除单位不同造成的特征之间的不平等。你可能会想，这组数据，我把用户收入除100，是不是也就把收入固定到了0~100的范围，跟年龄差不多？这确实也是一种处理办法，但是，这样的处理方法，一方面，量纲的影响还是没有完全统一，范围并没有填满。另一方面，用户收入这个特征的很多信息其实就丢失了，拿去机器学习就会丢失很多特征，效果就不会太好。那业界比较常用的方式有两种，归一化 和 标准化。")]),s._v(" "),a("p",[a("strong",[s._v("1、归一化")])]),s._v(" "),a("p",[s._v("归一化通过对原始数据进行变换把数据映射到[0,1]这样一个标准区间。而这个标准区间是可以根据实际情况调整的。")]),s._v(" "),a("p",[s._v("​\t他的标准计算公式如下：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175459836.png",alt:"image-20220624175459836"}})]),s._v(" "),a("blockquote",[a("p",[s._v("其中max,min分别表示这一列特征值中的最大值和最小值。 而mx,mi表示指定的映射区间。默认mx为1，mi为0。")]),s._v(" "),a("p",[s._v("归一化的缺点：对异常值敏感。当数据集中出现一个不太合理的极大值或者极小值时，整个归一化的结果就非常不好。鲁棒性(稳定性)较差")])]),s._v(" "),a("blockquote",[a("p",[s._v("sklearn方法： sklearn.preprocessing.MinMaxScaler")]),s._v(" "),a("p",[s._v("spark示例：JavaMinMaxScalerExample")])]),s._v(" "),a("p",[a("strong",[s._v("2、标准化")])]),s._v(" "),a("p",[s._v("​\t前面提到，归一化对异常值是非常敏感的。而标准化就能很好的处理这种异常数据的问题。")]),s._v(" "),a("p",[s._v("​\t标准化是通过对原始数据进行变换，把数据变换到均值为0，标准差为1的范围内。")]),s._v(" "),a("p",[s._v("​\t他的计算公式是：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("x'= (x-mean)/std\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("blockquote",[a("p",[s._v("mean： 特征值的均值， std：标准差。 均方差。")]),s._v(" "),a("p",[s._v("这种方式，对于极大或极小的少量异常点，均值和标准差都会比较稳定，所以异常值的影响就变小了。")])]),s._v(" "),a("blockquote",[a("p",[s._v("sklearn方法： sklearn.preprocessing.StandardScaler")]),s._v(" "),a("p",[s._v("spark示例：JavaStandardScalerExample")])]),s._v(" "),a("h4",{attrs:{id:"_3、降维"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、降维"}},[s._v("#")]),s._v(" 3、降维")]),s._v(" "),a("p",[s._v("​\t经过前面一通处理，大家是不是觉得数据变得越来越复杂了？特征值即变多了，也变得难看了。那怎么把这些数据简化一下呢？这就是后面的降维操作了。")]),s._v(" "),a("p",[s._v('这里的降维是什么？跟三体里的降维打击不是同一个东西。我们这里的降维是指在某些限定条件下，降低特征的个数，得到一组"不相关"的主变量的过程。那什么叫"不相关"呢？我们先简单的理解下什么叫特征与特征相关。例如，我们需要去学习某一个地区的降雨量，就会去统计一些常用的天气特征。而这其中，相对湿度与降雨量就是一个相关的特征，相对湿度大，肯定降雨量就会偏大。在进行机器学习训练算法时，如果特征本身存在问题或者特征之间相关性较强，那对于算法学习预测的影响就会比较大。而我们将为的过程，不光是要降低特征值的个数，同时也要尽量保留不相关的特征。')]),s._v(" "),a("p",[s._v("​\t降维主要有两种方式：")]),s._v(" "),a("p",[a("strong",[s._v("1>特征选择")])]),s._v(" "),a("p",[s._v("​\t数据中包含了冗余或者相关变量时，通过数学方法从原有特征中找出主要的特征。选择的方式也有两个")]),s._v(" "),a("ul",[a("li",[s._v("方差选择法： 过滤低方差特征(过于集中的数据)")])]),s._v(" "),a("blockquote",[a("p",[s._v("在sklearn中封装了一个函数:  sklearn.feature_selection.VarianceThreshold 可以根据预设的阈值过滤数据集中的特征值。")]),s._v(" "),a("p",[s._v("在Spark中没有找到对应的示例，这里就不再去深究了。")])]),s._v(" "),a("ul",[a("li",[s._v("相关系数法：特征与特征之间的相关程度。例如 天气湿度 与 降雨量 一般就认为是相关性很强的特征。")])]),s._v(" "),a("p",[s._v("Spark可以查看示例：JavaCorrelationExample  。里面计算了皮尔逊距离和斯皮尔曼距离的矩阵。关于相关性，如果相关性为正数，表示两个特征正相关，即一个特征值越大，另一个特征值也越大。而如果是负数，就表示两个特征负相关。另外计算出来的关系系数的绝对值越大，表示特征的关联性更强。对于相关的特征，可以再考虑将他们组合成一个新的特征。")]),s._v(" "),a("p",[s._v("这个里面涉及到大量的数学计算，这节课就不给大家演示了。有兴趣大家可以课后深入了解。")]),s._v(" "),a("p",[a("strong",[s._v("2>主成分分析")])]),s._v(" "),a("p",[s._v("定义：一种将高维数据转换为低维数据的方法。保留对目标值结果影响较大的特征值，去掉相对影响较小的特征值。")]),s._v(" "),a("p",[s._v("作用：数据维度压缩，尽可能降低原数据的维数(特征个数)，损失少量的信息。")]),s._v(" "),a("p",[s._v("理解：将高纬度数据投影到低纬度空间的过程。例如，一个(x,y)两维坐标点可以投影到一条y=f(x)的一维直线上，或者像皮影戏一样，将一个三维物体投影到一个二维平面上。而损失的信息量，可以简单的理解为能不能从低维还原成高维。比如照一张照片，从照片能不能推断出原来是个什么东西。 这个过程跟三体里的降维打击就有点相同了，只是是一种数学上的表达方方式。")]),s._v(" "),a("p",[s._v("​\t高深的解释太复杂，最简单的理解就是特征个数太多，不好分析时，就可以用PCA来减少特征个数。")]),s._v(" "),a("blockquote",[a("p",[s._v("sklearn方法： sklearn. decoposition.PCA")]),s._v(" "),a("p",[s._v("spark示例：JavaPCAExample")])]),s._v(" "),a("h3",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[s._v("#")]),s._v(" 总结")]),s._v(" "),a("p",[s._v("​\t这一节课的目标是带大家了解机器学习领域一些耳熟能详的基础工程方法。结合下一节课的基础算法后，大家就可以上kaggle去逛一逛，看一些简单的例子了。这样在接下来设计推荐系统时，就能更加得心应手。")]),s._v(" "),a("blockquote",[a("p",[s._v("特征工程其实并不复杂，就是对数据进行处理。如果大家有兴趣想要更深入的了解下机器学习，给大家推荐一个kaggle上的入门教程： https://www.kaggle.com/ialimustufa/titanic-beginner-s-guide-with-sklearn 。你只需要简单的了解下Python的基础语法，这整个过程还是可以跟下来的。感受下简单的特征工程是怎么做的。如果你能够把整个过程跟下来，可以再尝试把我们这节课的特征工程算法给加上去试试。")]),s._v(" "),a("p",[s._v("特征工程却是机器学习最为重要的工作，也是机器学习算法工程师最经常做的事情(还有一个就是调算法的参数)。而对于Kaggle上的机器学习竞赛，拉开差距的地方也就在特征工程。好的特征工程能够让机器学习算法发挥更大的效果，更加逼近特征与数据带来学习效果的上限。这一节给大家介绍的几种特征工程算法是经过很多机器学习过程验证过之后，业界普遍认为效果比较好的特征工程处理方式。")]),s._v(" "),a("p",[s._v("而通过这一节课，我们绕开了那些复杂的基础知识，直接带大家体验了一下特征工程是怎么做的。并且，其实在机器学习领域，大家应该能够发现，java语言还是太重了，所以一般还是python或者R语言这种直接一上来就做事的语言跟更适合。但是这节，也带大家了解了Spark中对机器学习算法的相关封装。再结合我们后面的大数据体系课程，相信机器学习离我们大家并不是太远。")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("U2FsdGVkX184Ke0xNOZtVltPkOVxdEt+Y0/fpXOR0yMeaXa1nQnBYGUk5m/t1lHc\nfxD9Os/yuj7BaI6PyOfI8V4Fy7UlQiBFIYr7lgT4Eg7AQBxgP8Cz3gT29iFbS+3R\n5A==\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h2",{attrs:{id:"五、机器学习-典型算法实现"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#五、机器学习-典型算法实现"}},[s._v("#")]),s._v(" 五、机器学习-典型算法实现")]),s._v(" "),a("p",[s._v("​\t这一节课，我们来了解一下常用的机器学习算法。目的是结合上一节的特征工程，让大家能够去kaggle上逛一逛，对机器学习入个门。然后我们再来用机器学习的方式来设计推荐系统。")]),s._v(" "),a("p",[s._v("机器学习适用的数据为具有特征值(属性，label)和目标值(结果,point)的数据集。通过从历史数据集中学习经验，建立模型，从而达到预测新特征值对应的目标值的效果。因此在数据方面，越见多识广的数据集(样本集越大越全)，越能进行更可信的预测(越准确的预测)。")]),s._v(" "),a("p",[s._v("算法分类：")]),s._v(" "),a("ul",[a("li",[s._v("按有无目标值值分类：1、监督学习：有目标值  2、无监督学习：无目标值")]),s._v(" "),a("li",[s._v("按目标值类型分类；1、分类算法：目标值为一组有限且固定的序列 2、回归算法：目标值为一个连续的数值空间")])]),s._v(" "),a("h3",{attrs:{id:"_1、knn算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、knn算法"}},[s._v("#")]),s._v(" 1、KNN算法")]),s._v(" "),a("p",[a("strong",[s._v("定义：")])]),s._v(" "),a("p",[s._v("​\t如果一个样本在特征空间中的K个最相似(距离最近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。人以类聚，物以群分。")]),s._v(" "),a("p",[a("strong",[s._v("距离计算公式：")])]),s._v(" "),a("p",[s._v("欧式距离(平方根距离)、曼哈顿距离(绝对值距离)，明科夫斯基距离(以上两种距离均是明科夫斯基距离的特例)")]),s._v(" "),a("p",[a("strong",[s._v("适用案例：")])]),s._v(" "),a("p",[s._v("iris，根据鸢尾花的一些特征判断一个鸢尾花所属的种类")]),s._v(" "),a("p",[s._v("适用于小数据场景，K-W数据量级别的样本。")]),s._v(" "),a("p",[a("strong",[s._v("算法优缺点：")])]),s._v(" "),a("ul",[a("li",[s._v("优点：简单、易于理解，易于实现，无需训练")]),s._v(" "),a("li",[s._v("缺点：1、懒惰算法，对预测样本分类时才进行计算，计算量大，内存开销大。\n2、必须指定K值，K值选择会极大程度影响分类的准确度。\n关于K值选取：K值过小，容易受到异常数据的影响。而K值过大，容易受样本不均衡的影响。")])]),s._v(" "),a("p",[a("strong",[s._v("特征工程处理：")])]),s._v(" "),a("p",[s._v("需尽量保证各个维度的数据公平性。无量纲化-标准化处理。尽量保证各个维度的数据公平性。")]),s._v(" "),a("p",[a("strong",[s._v("skLearn API:")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("sklearn.neighbors.KNeighborsClassifier(n_neighbor=5,algrithm='auto')\n\t\tn_neighbors: int 可选，默认5 ， K值\n\t\talgorithm : {'auto','ball_tree','kd_tree','brute'} .可选。用于计算最近的算法。有ball_tree、kd_tree。不同的实现方式会影响效率，但不影响结果。一般用auto，会自己根据fit方法的值来选择合适的算法。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[a("strong",[s._v("spark Demo:")])]),s._v(" "),a("p",[s._v("​\t未找到。")]),s._v(" "),a("blockquote",[a("p",[s._v("从这个示例重点理解机器学习的基础处理流程。然后理解对K值参数的调优对结果的影响。")])]),s._v(" "),a("h3",{attrs:{id:"_2、朴素贝叶斯分类算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、朴素贝叶斯分类算法"}},[s._v("#")]),s._v(" 2、朴素贝叶斯分类算法")]),s._v(" "),a("p",[s._v("朴素：假定了特征与特征之间相互独立，没有影响。")]),s._v(" "),a("p",[s._v("贝叶斯：贝叶斯概率计算公式。")]),s._v(" "),a("p",[a("strong",[s._v("原理：")])]),s._v(" "),a("p",[s._v("朴素的概率计算：30%的男人是好人，80%的老人是好人， 那男老人有24%的概率是好人。")]),s._v(" "),a("blockquote",[a("p",[s._v("概率基础知识：独立的定义 P(A,B) = P(A)*P(B)")]),s._v(" "),a("p",[s._v("全概率公式： P(A)=P(A|B1)*P(B1)+P(A|B2)*P(B2)+P(A|B3)*P(B3)+P(A|B4)*P(B4)+。。。。")])]),s._v(" "),a("p",[a("strong",[s._v("应用场景：")])]),s._v(" "),a("p",[s._v("文本分类、评论区分好差评")]),s._v(" "),a("p",[a("strong",[s._v("优缺点：")])]),s._v(" "),a("p",[s._v("优点：发源于古典数学理论，有稳定的分类效率。")]),s._v(" "),a("p",[s._v("​\t对缺失数据不敏感，算法也比较简单，常用语文本分类。")]),s._v(" "),a("p",[s._v("​\t分类速度快，准确度相对较高。")]),s._v(" "),a("p",[s._v("缺点：由于使用了样本属性独立的假设，当特征属性之间有关联时，其效果就不太好。")]),s._v(" "),a("p",[s._v("​\t基于概率论的简单统计，样本数量越大越好。")]),s._v(" "),a("p",[a("strong",[s._v("Sklearn API:")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\tsklearn.naive_bayes.MuitinomialNB(alpha = 1.0)\n\talpha:拉普拉斯平滑系数，一般用1 。用来防止计算出的概率为0.使用方式是在分子和分母上一起添加平滑系数。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[a("strong",[s._v("spark 示例：")])]),s._v(" "),a("p",[s._v("​\t JavaNaiveBayesExample")]),s._v(" "),a("h3",{attrs:{id:"_3、决策树"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、决策树"}},[s._v("#")]),s._v(" 3、决策树")]),s._v(" "),a("p",[s._v("决策树是用来解决分类问题的。决策树模型就是一个多层的if-else结构。")]),s._v(" "),a("p",[a("img",{attrs:{src:"http://null/",alt:"img"}})]),s._v(" "),a("p",[a("strong",[s._v("分类原理：")])]),s._v(" "),a("p",[s._v("​\t决策树的关键就是判断的关键特征的先后顺序，要能尽量快速过滤掉大部分不符合标准的情况。")]),s._v(" "),a("p",[s._v("决策树的划分依据之一：信息熵 entropy，信息增益。信息熵可认为是信息的混乱程度。而信息增益可以理解为按某一个属性进行划分后的信息熵增益。决策树的划分方式就是在每一个节点选择信息增益最大的属性进行划分。")]),s._v(" "),a("p",[s._v("​\t关于信息熵和信息增益的计算，在数学上有明确的计算公式，但是这里就不去过多推演了。")]),s._v(" "),a("p",[a("strong",[s._v("SkLearn API")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\tsklearn.tree.DecisionTreClassifier(criterion='gini',max_depth=None,random_state=None)\n\t决策树分类器\n\tcriterion: 默认是'gini'系数，也可以选择信息增益的熵‘entropy’\n\tmax_depth:树的深度大小\n\trandom_state:随机数种子\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("blockquote",[a("p",[s._v("熵的计算比较复杂，默认会使用相对比较简单的gini系数。gini系数可以理解为是信息熵的一个近似结果。")])]),s._v(" "),a("p",[a("strong",[s._v("spark Demo:")])]),s._v(" "),a("p",[s._v("JavaDecisionTreeClassificationExample、JavaDecisionTreeRegressionExample")]),s._v(" "),a("p",[a("strong",[s._v("优缺点：")])]),s._v(" "),a("p",[s._v("​\t优点： 决策树模型的可解释能力强，不需要进行数据归一。模型就是一个多层的If-else结构，比较容易理解。\n​\t缺点：每个决策边界只能涉及到一个特征，不太容易扩展到更复杂的情况，整个决策树容易过大。树的深度过大就容易产生过拟合。")]),s._v(" "),a("p",[s._v("过拟合的调整方式是配置maxDepth参数，修改树的深度。改进方法是进行剪枝，防止整个树过于庞大。另一种比较好的方式就是采用随机森林")]),s._v(" "),a("h3",{attrs:{id:"_4、随机森林"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、随机森林"}},[s._v("#")]),s._v(" 4、随机森林")]),s._v(" "),a("p",[s._v("​\t通过建立几个模型组合来解决单一预测问题。其原理就是生成多个分类器(模型)，各自独立地学习和做出预测。这样预测最后结合成组合预测，一般都优于任何一个单一分类器做出的预测。其输出的类别就是取多数的结果。")]),s._v(" "),a("p",[a("strong",[s._v("原理：")])]),s._v(" "),a("p",[s._v("​    BootStrap随机有放回抽样。从训练集中随机取一个样本，放入新训练集。然后从原训练集中再随机抽取(已抽取的样本放回原训练集)。这样，每棵树的训练集都是独有的。\n​    相当于将数据分散成不同的树。随机的预测思想就是这些树中有“正确”的树，也有“错误”的树。而“错误”的树的学习结果会互相抵消，最终“正确”的树就会脱颖而出。")]),s._v(" "),a("p",[a("strong",[s._v("sklearn API：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None,min_samples_split=2)\n\tn_estimators: Int 默认10 森林里的树木数量\n\tcriteria： String  默认gini . 分隔特征的测量方法  entropy\n\tmax_depth: Integer或者None,可选。树的最大深度\n\tmax_features=\"auto\" 每个决策树的最大特征数量 \n\t\tauto,  sqrt  log2   None\n\t\tauto和sqrt是一个意思，开根号。 None取跟原样本一样的特征树。\n\tbootstrap: boolean 默认true 是否在构建树时使用放回抽样\n\tmin_samples_split:节点划分最少样本数\n\tmin_samples_leaf:叶子节点的最小样本数\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("p",[s._v("超参数：n_estimator, max_depth,min_samples_split,min_samples_leaf。")]),s._v(" "),a("p",[a("strong",[s._v("Spark Demo:")])]),s._v(" "),a("p",[s._v("JavaRandomForestClassifierExample")]),s._v(" "),a("p",[a("strong",[s._v("优缺点：")])]),s._v(" "),a("p",[s._v("​\t在所有分类算法中，具有较好的准确率。能够有效的运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维。能够评估各个特征在分类问题上的重要性。")]),s._v(" "),a("h3",{attrs:{id:"_5、线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5、线性回归"}},[s._v("#")]),s._v(" 5、线性回归")]),s._v(" "),a("p",[s._v("​\t回归问题：目标值是一组连续的数据。找到一种函数关系，来表示特征值与目标值之间的关系。")]),s._v(" "),a("p",[s._v("​\t而线性回归就是以一个多元一次函数的方式来尽量的逼近所有的目标点。")]),s._v(" "),a("p",[s._v("​\t函数形式：   y=f(x)=w_1"),a("em",[s._v("x_1+w_2")]),s._v("x_2+w_3"),a("em",[s._v("x_3+.....+b。 如果是二维y=w_1")]),s._v("x_1+b的话，大家应该非常熟悉，在坐标系上就是一条线，所以叫线性回归。")]),s._v(" "),a("p",[s._v("​\t线性回归问题的关键就是要找出一组最适合的参数值w(形式为一个矩阵)和一个偏移量b。")]),s._v(" "),a("p",[a("strong",[s._v("原理：")])]),s._v(" "),a("p",[s._v("用一个多元一次方程来描述特征值与目标值之间的关系，线性关系。")]),s._v(" "),a("p",[s._v("使用场景：波士顿房价预测Demo")]),s._v(" "),a("p",[a("strong",[s._v("目标：")])]),s._v(" "),a("p",[s._v("模型参数能够尽量准确的预测目标值。--损失函数最小。")]),s._v(" "),a("p",[a("strong",[s._v("SKlearn API：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("--正规方程线性回归\nsklearn.linear_model.LinearRegression(fit_interfcept=True)\n- 通过正规方程优化\n- fit_intercept： 是否计算偏置--参数序列最后的b\n- LinearRegression.coef_ ： 回归系数\n- LinearRegression.intercept_ ： 偏置\n\n--随机梯度下降线性回归\nsklearn.linear_model.SGDRegressor(loss=\"squard_loss\",fit_intercept=True,learning_rate='invscaling',eta0=0.01)\n- SGDRegressor类实现了随机梯度下降学习，支持不同的loss函数和正则化惩罚项来拟合线性回归模型\n- loss: 损失类型  squared_loss 普通最小二乘法\n- fit_intercept: 是否计算偏置\n- learning_rate: string,可选项： \n  \tconstant : eta=eta0 \n  \toptimal: eta=1.0/(alpha*(t+t0)) --default\n  \tinvscaling.: eta = eta0/pow(t,power_t)\n  \t常用constant\n  返回结果\n  SGDRegressor.coef_ ：回归系数\n  SGDRegressor.intercept_ ：偏置\n \n --计算均方误差\n sklearn.metrics.mean_squared_error(y_true,y_predict)\n return  浮点数结果\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br")])]),a("p",[a("strong",[s._v("SparkDemo:")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("JavaLinearSVCExample，计算正规方程。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("​\tJavaLinearRegressionWithElasticNetExample：包含了误差计算。这个ElasticNet就是线性回归用于优化损失函数的一种方式。")]),s._v(" "),a("p",[a("strong",[s._v("正规方程与线性回归")])]),s._v(" "),a("p",[s._v("​\t在线性回归问题中，机器学习的目标就是不断减少损失函数。而优化损失函数的方法有两种：")]),s._v(" "),a("ul",[a("li",[s._v("一种是像我们计算二元一次方程组一样，直接用公式去计算最佳的一组解。称为正规方程。这种方式不需要学习，直接求解。但是运算在大数据量下的计算会非常复杂，通常只适用于小数据量。")]),s._v(" "),a("li",[s._v("另一种是先假定一组解，然后不断试错，慢慢逼近正确答案。称之为梯度下降。")])]),s._v(" "),a("p",[s._v("几种梯度下降的优化方法：")]),s._v(" "),a("p",[s._v("GD： Grandient Descent ; 原始的梯度下降方式。")]),s._v(" "),a("p",[s._v("SGD：Stochastic Grandient Descent：随机梯度下降。比较高效，节省时间。缺点是需要许多超参数，对特征标准化敏感。")]),s._v(" "),a("p",[s._v("SAG：Stochastic Average Gradient 。 随机平均梯度法。比SGD的收敛速度更快")]),s._v(" "),a("blockquote",[a("p",[s._v("spark示例： JavaSVMWithSGDExample")])]),s._v(" "),a("p",[a("strong",[s._v("模型评估")])]),s._v(" "),a("p",[s._v("​\t计算损失函数的方法也是有很多的，比如均方误差RMSE，MSE，R2等。这些参数都可以用来对线性回归模型进行评测。")]),s._v(" "),a("p",[a("strong",[s._v("过拟合与欠拟合")])]),s._v(" "),a("p",[s._v("这也是机器学习过程中最为核心的问题。正规方程直接计算出最佳的结果，是不是就是最好的？其实也不是，数据之间的规律并不是稳定的，一般就不存在绝对的规律。机器学习的目的其实是要能够去对未来的数据进行预测，是一种模糊的行为。正规方程还有一个最重要的问题，就是他通常在训练集上表现良好，但是到验证集上表现就会差很多。这样的模型泛化能力不够，不能很好的体现未来数据的特征。这种问题就是"),a("strong",[s._v("过拟合现象")]),s._v("，即机器学习从数据集中学到的规律过多。就像我们平常说的书呆子，学少了不好，学太多了同样也不好。")]),s._v(" "),a("p",[s._v("​\t另外还有一种情况，如果数据的样本比较少，那机器学习学到的规律也会太不靠谱了。这就称为"),a("strong",[s._v("欠拟合现象")]),s._v("。通常表现为算法在训练集和测试集上的表现都不太好。这就像我们常说的学渣，就是学习还不够。")]),s._v(" "),a("p",[s._v("​\t过拟合和欠拟合现象是机器学习过程中绕不开的问题。而算法工程师很多的工作就是要综合评测算法和参数，在过拟合和欠拟合之间寻找最佳解。通常，对于欠拟合现象，可以通过加数据、加特征等手段来优化。而过拟合的优化就比较麻烦，通常需要做一些针对性的特征工程。")]),s._v(" "),a("h3",{attrs:{id:"_6-逻辑回归与二分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-逻辑回归与二分类"}},[s._v("#")]),s._v(" 6：逻辑回归与二分类")]),s._v(" "),a("p",[s._v("二分类问题： 是否垃圾邮件、是否金融诈骗、是否虚假帐号。。。\n这里即是用逻辑回归来解决二分类问题。")]),s._v(" "),a("p",[s._v("逻辑回归原理：")]),s._v(" "),a("p",[s._v("先对一组数据建立线性回归模型，h(w) = w1"),a("em",[s._v("x1 + w2")]),s._v("x2+...+b\n然后用线性回归的输出作为逻辑回归的输入，将特征值映射到一个分类中，作为预测的目标值。例如：sigmoid激活函数：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175628755.png",alt:"image-20220624175628755"}})]),s._v(" "),a("p",[s._v("线性回归的输出作为逻辑回归的输入")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175636309.png",alt:"image-20220624175636309"}})]),s._v(" "),a("p",[s._v("例如，看下图的计算示例，逻辑回归的结果可以认为是样本的二分类概率。然后，同样通过损失函数来计算逻辑回归的模型性能。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175650363.png",alt:"image-20220624175650363"}})]),s._v(" "),a("p",[a("strong",[s._v("逻辑回归API：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\tsklearn.linear_model.LogisticRegrsssion(solver='liblinear',penalty='l2',C=1.0)\n- solver: 优化求解模式。 默认 liblinear , 还有sag 随机平均梯度下降\n- penalty: 正则化的种类。 l1 l2\n- C: 正则化力度\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[a("strong",[s._v("SparkDemo:")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("JavaLogisticRegressionSummaryExample，\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("​\tJavaLogisticRegressionWithLBFGSExample")]),s._v(" "),a("p",[a("strong",[s._v("逻辑回归模型评估：")])]),s._v(" "),a("p",[s._v("​    逻辑回归的结果只是一个二分类的概率，如有80%的可能是垃圾邮件。然而这种概率结果其实是很虚的，用模型评估拿到也是一个概率，闭上眼睛都选C也是一种概率。那要怎么评估模型的可信度呢？")]),s._v(" "),a("p",[a("strong",[s._v("混淆矩阵、精确率(Precision)、召回率(Recall)")])]),s._v(" "),a("p",[s._v("在二分类任务中，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵(这个矩阵其实在多分类问题中也能建立，只是更加复杂)，通过混淆矩阵，可以对分类结果从多个不同的方面来进行评价。")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"left"}},[s._v("真实结果\\预测结果")]),s._v(" "),a("th",[s._v("正例")]),s._v(" "),a("th",[s._v("假例")])])]),s._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"left"}},[s._v("正例")]),s._v(" "),a("td",[s._v("真正例TP")]),s._v(" "),a("td",[s._v("为反例FN")])]),s._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"left"}},[s._v("假例")]),s._v(" "),a("td",[s._v("伪正例FP")]),s._v(" "),a("td",[s._v("真反例TN")])])])]),s._v(" "),a("p",[a("strong",[s._v("精确率Precision")]),s._v("： 预测结果为正例样本中真实为正例的比例：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("一批西瓜中，预测为好西瓜的瓜有多少真正是好西瓜。体现模型的准确性。--要把好西瓜挑出来\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175921365.png",alt:"image-20220624175921365"}})]),s._v(" "),a("p",[a("strong",[s._v("召回率Recall")]),s._v("：真实为正例的样本中预测结果也为正例的比例：对正样本的区分能力。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("一批西瓜中，所有真正为好西瓜的西瓜中有多少是被正确预测为好西瓜。体现模型对正样本的区分能力。--要把坏西瓜扔掉\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175932080.png",alt:"image-20220624175932080"}})]),s._v(" "),a("p",[s._v("另外，还有其他更复杂的评估标准。如F1-Score，反映了模型的稳健型")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175939015.png",alt:"image-20220624175939015"}})]),s._v(" "),a("p",[a("strong",[s._v("混淆矩阵计算API：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("`sklearn.metrics.classification_report(y_true,y_pred,labels=[],target_name=None)`\n- `y_true : 真实目标值数组`\n- `y_pred: 估计器预测目标值`\n- `labels: 指定类别对应的数字`\n- `target_names: 目标类别名称`\n- `return： 每个类别精确率与召回率`\n  - `precision 精确率`\n  - `recall 召回率`\n  - `f1-scoreL 稳健度`\n  - `support 样本数`\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("p",[s._v("​    有了这些指标后，能够一定程度上评估模型的健康度。但是光有这些指标还不太够，例如在样本不均衡，正样本太多，负样本太少时，这些指标评估结果就不太可信。而为了能更准确的评估二分类模型，就引入了ROC曲线和AOC指标。")]),s._v(" "),a("p",[a("strong",[s._v("ROC曲线和AUC指标")])]),s._v(" "),a("p",[s._v("首先需要了解TPRate和FPRate")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("TPRate = TP / (TP+FN): 所有真实类别为1的样本中，预测类别也为1的比例")])]),s._v(" "),a("li",[a("p",[s._v("FPRate = FP/( FP + TN):所有真实类别为0的样本中，预测类别为1的比例")]),s._v(" "),a("p",[s._v("有这两个数据后，对于每一个分类器，就可以建立一条ROC曲线")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175948923.png",alt:"image-20220624175948923"}})]),s._v(" "),a("p",[s._v("而AUC指标就可以认为是ROC曲线下方的图形面积。")]),s._v(" "),a("p",[s._v("因此，")])]),s._v(" "),a("li",[a("p",[s._v("AUC指标的概率意义是随机取一对正负样本，正样本得分大于负样本的概率。")])]),s._v(" "),a("li",[a("p",[s._v("AUC指标的最小值是0.5，最大值是1，取值越大越好")])]),s._v(" "),a("li",[a("p",[s._v("AUC=1，就是完美分类器，采用这个预测模型时，不管设定什么阈值都能的出完美预测结果。但是，在绝大多数预测的场合，都不存在完美分类器。")])]),s._v(" "),a("li",[a("p",[s._v("0.5<AUC<1,优于随机猜测。这个分类器(模型)妥善设定阈值的话，能有预测价值。")])]),s._v(" "),a("li",[a("p",[s._v("如果AUC<0.5，就会采用1-AUC来表示AUC的值，代表反向预测。")])]),s._v(" "),a("li",[a("p",[s._v("同时，AUC指标也能用于比较多个不同的分类器的性能。")])])]),s._v(" "),a("p",[a("strong",[s._v("AUC指标计算API：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\t`sklearn.metrics.roc_auc_score(y_true,y_score)`\n- `计算ROC曲线面积，即AUC值`\n- `y_true: 每个样本的真是类别，必须为0-反例，1-正例 标记`\n- `y_score:预测得分，可以是正类的估计概率、可信值或者分类器方法的返回值`\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[a("strong",[s._v("Spark计算AUC示例：")])]),s._v(" "),a("p",[s._v("见JavaLBFGSExample中一段示例代码")]),s._v(" "),a("div",{staticClass:"language-java line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// Get evaluation metrics.")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("BinaryClassificationMetrics")]),s._v(" metrics "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("new")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("BinaryClassificationMetrics")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("scoreAndLabels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("rdd")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("double")]),s._v(" auROC "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" metrics"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("areaUnderROC")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[a("strong",[s._v("AUC总结")])]),s._v(" "),a("ul",[a("li",[s._v("AUC只能用来评估二分类问题")]),s._v(" "),a("li",[s._v("AUC非常适合评价样本不均衡时的分类器性能。")])]),s._v(" "),a("h3",{attrs:{id:"_7-无监督学习-k-means算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-无监督学习-k-means算法"}},[s._v("#")]),s._v(" 7： 无监督学习-K-means算法")]),s._v(" "),a("p",[s._v("无监督学习：无目标值\n示例场景：一家广告平台需要根据相似的人口学特征和客户消费习惯将目标客户分成不同的小组，以便广告客户可以通过有关联的广告解除到他们的目标客户")]),s._v(" "),a("p",[s._v("算法：K-Means聚类， PCA(主成分分析法)降维")]),s._v(" "),a("p",[s._v("以下KMeans效果图： 将一群点随机分成三类")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624175958849.png",alt:"image-20220624175958849"}})]),s._v(" "),a("p",[s._v("而下面这个图说明了KMeans的计算步骤：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624180007964.png",alt:"image-20220624180007964"}})]),s._v(" "),a("p",[a("strong",[s._v("KMeans API:")])]),s._v(" "),a("p",[s._v("kMeans实现走预估器代码流程")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("sklearn.cluster.KMeans(n_clusters=8, init = 'k-means++')\nn_clusters:开始的聚类中心数量\ninit : 初始化方法。\nlabels_ : 默认标记的类型，可以和真实值比较(不是值比较)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[a("strong",[s._v("SparkDemo:")])]),s._v(" "),a("p",[s._v("​\tml.JavaKMeansExample")]),s._v(" "),a("p",[s._v("​\tmllib.JavaKMeansExample")]),s._v(" "),a("p",[a("strong",[s._v("kMeans性能评估")]),s._v("：")]),s._v(" "),a("p",[s._v("KMeans采用轮廓系数来进行评估，轮廓系数在-1，1之间，越接近1，分类效果越好，越接近-1 ，分类效果越差。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624180016246.png",alt:"image-20220624180016246"}})]),s._v(" "),a("p",[a("strong",[s._v("轮廓系数API：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("sklearn.metrics.silhouette_score(X,labels)`\n计算所有样本的平均轮廓系数`\nX: 特征值`\nlabels: 被聚类标记的目标值\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("在Spark的ml包下的JavaKMeansExample中也有计算轮廓系数的方式。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('ClusteringEvaluator evaluator = new ClusteringEvaluator();\ndouble silhouette = evaluator.evaluate(predictions);\nSystem.out.println("Silhouette with squared euclidean distance = " + silhouette);\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[a("strong",[s._v("KMeans特点：")])]),s._v(" "),a("p",[s._v("特点： KMeans采用迭代式算法，直观易懂并且非常实用\n缺点：如果样本分布不均匀，且一开始随机选取的聚类中心过于局部集中 ，就容易被收敛到局部最优解。解决办法：可以多次聚类，多随机选取几次初始中心点来解决。\n应用场景：聚类一般用在分类之前。没有目标值，先用聚类计算出一部分目标值，再用目标值进行分类。")]),s._v(" "),a("h3",{attrs:{id:"总结-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结-2"}},[s._v("#")]),s._v(" 总结")]),s._v(" "),a("p",[s._v("​\t到这里为止，机器学习一些经典的工具和算法就给大家介绍完了。这一部分主要是给大家在机器学习领域入个门，对于一些比较基础的机器学习问题，不说一定就能做出来，至少有一些想法了把。")]),s._v(" "),a("p",[s._v("在最开始介绍推荐系统问题时提到过，我们之间简单的按照物品销售量排名，其实也算是一个推荐系统，但是他的坏处是没有考虑到个体的差异，也就是没有利用到推荐矩阵中已有的历史记录。而通过这些机器学习的方法，就可以更深入的挖掘已有的数据中的潜在规律，制定更好的推荐策略。下一章节我们就会回归到推荐系统的问题，看看如何用机器学习来优化推荐系统。")]),s._v(" "),a("p",[s._v("最后，其实我们这里介绍到的机器学习，还是属于传统的机器学习部分，大都属于之前介绍过的符号主义。他们有一个比较共同的特点，就是机器学习的过程其实都是一些严格的数学计算，也就是说，只要我们理论知识足够扎实，这些机器学习算法的整个过程和结论，都是我们可以完全掌握的。但是，其实还有一大类的问题，已经超出了我们理论能够理解的范畴。比如AlphaGo下围棋，如果科学家能够完全掌握AlphaGo学习到的算法，那就完全可以自己学习AlphaGo，超过所有专业围棋手了对吧。还有比如现在很火的DeepMind，已经在星际争霸等游戏中开始用机器学习来进行模拟改进了。在这样一些领域，我们的这些机器学习算法就非常乏力了。这时候就需要更有力的机器学习算法才行，这就是"),a("strong",[s._v("深度学习")]),s._v("。也就是之前给大家介绍过的花书的内容。这些就有待大家深入探索了。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("U2FsdGVkX18SkMkopnaCTxpp6SyfCauHwpSLkQNiynksdEU6E88hae6YK2JJZU9H\nmSfS8vnf85Kg1rK3PoFYGFFSPSjh8p66wzJshCwTp2cM/wBVm1yA0p4YjgdVYxdr\nhg==\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("h2",{attrs:{id:"六、推荐系统设计与改进"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#六、推荐系统设计与改进"}},[s._v("#")]),s._v(" 六、推荐系统设计与改进")]),s._v(" "),a("p",[s._v("​\t这一章节回到我们的推荐系统问题。其实经过前面的总结，应该不难知道，推荐系统这样一个典型问题，其实按照他对于推荐规律的不同理解程度，可以分为以下四种设计方式：")]),s._v(" "),a("ul",[a("li",[a("p",[s._v("基于统计学的推荐系统：")]),s._v(" "),a("p",[s._v("我们之前基于商品销量排行的推荐系统就是一种典型的基于统计学的推荐系统。这类推荐系统实现比较简单，但是千篇一律，所有人看到的东西都一样，不利于深入挖掘用户与产品之间的关系，更适用于传统购物场景。")])]),s._v(" "),a("li",[a("p",[s._v("基于规则的推荐系统：")]),s._v(" "),a("p",[s._v("可以通过一些简单的业务规则来指定推荐系统的实现方式。例如，一个电影院可以简单的把动画电影主推给青少年，把战争大片主推给年轻人，而把爱情片主推给中老年人。这种推荐系统，更多的是由业务来进行驱动，软件层面实施起来相对就会非常简单。比较适用于时间比较短，用户比较少，数据量规模也比较小的电商场景。")])]),s._v(" "),a("li",[a("p",[s._v("基于传统机器学习的推荐系统：")]),s._v(" "),a("p",[s._v("当电商的整体规模逐渐扩大，全方位的广告覆盖就会变得得不偿失。这个时候就需要更深度的挖掘历史数据，深入分析用户与产品之间的潜在关系，进行更据针对性，更有效的产品推荐。这时候，机器学习算法就能更好的帮我们寻找处理和计算这些历史数据的方式，并且通过建立数学模型，对未来的交易行为进行一定的预测。这也是我们会主要讨论的推荐系统。")])]),s._v(" "),a("li",[a("p",[s._v("基于深度学习的推荐系统：")]),s._v(" "),a("p",[s._v("但是，当电商的规模继续扩大，或者需要考虑同行业合作这种数据量更大的场景时，数据不光是体量增加，数据之间的关系也会变得更为复杂，这时基于数学计算的传统机器学习算法依然会表现得比较乏力，甚至于可以说人的理解能力也都会成为限制。这时就需要更为强力的基于神经网络的深度学习算法来加强机器学习的能力。这些深度学习算法其本质虽然依然是数学计算，但是他是在模拟人脑的结构以及处理问题的方式，可以理解为具备了一定的自我理解与自我演化的能力。认为是未来的推荐系统，目前也有很多大企业在使用像AlphaGo这样可以自我演化的推荐系统，但是在行业内还并没有形成规模。到时候，可能我们以前经常说的，你在淘宝买了块瑜伽垫，我马上敲门给你推荐减肥药这样的段子，也就会成为常态。")])])]),s._v(" "),a("p",[s._v("​\t我们这一章节主要讨论第三种，基于传统机器学习的推荐系统应该要如何进行核心的历史数据计算以及要如何设计系统的整体方案。")]),s._v(" "),a("h3",{attrs:{id:"_1、数据处理问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、数据处理问题"}},[s._v("#")]),s._v(" 1、数据处理问题")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("用户画像：")])]),s._v(" "),a("p",[s._v("关于推荐系统的数据要求，在之前已经简单做过介绍。推荐学习算法需要的数据是以(userId,ProductId,Rating)这样的向量数据的格式组成的一个稀疏矩阵。其中Rating表示用户对产品的评分。用来表示一个推荐的结果。可以是0和1这样的二分类结果集，也可以是一个有范围的正整数，表示用户对产品的评分结果集。而userId，productId，这两个关键属性在机器学习算法中，只是作为一个与具体对象特征无关的一个代表值。这样确实也可以根据用户的历史行为数据做出一个比价好的推荐结果。但是，你可能会觉得，这样的推荐系统，跟用户或者产品的一些具体属性就脱节了。")]),s._v(" "),a("p",[s._v("所以，往往在对推荐系统进行业务优化的过程中，会将这个userId和productId替换成对用户特征有一定描述性的特征值。例如，对用户进行特征画像，比如将用户按照年龄阶段，划分为0-老年人、1-中年人和2-青少年三个值，将用户按照年收入情况划分为0-高产、1-中产以及2-低产三个值等等，这个过程就是用户画像的过程。用户画像往往是机器学习的基础。在前面的章节提到过，对机器学习来说，数据和特征就决定了机器学习效果的上限，所以，通过用户画像对机器学习的数据进行及时适当的梳理和调整，是整个机器学习的基础。")]),s._v(" "),a("p",[s._v("当然，在推荐场景，一方面，要对画像的各种结果进行收敛，将所有的特征值整合成一个具有业务意义的userId，这样就可以用来进行机器学习了。比如，像支付宝那样，将用户的所有相关数据整合成一个蚂蚁信用分，然后用这样的信用分来进行学习，就能形成更具有明确业务意义的推荐系统。这样的结果，他的业务解释性也会更强。当然，这里需要注意一下用户区分度的问题，很多用户都可能有相同的分数。另一方面，其实画像也不仅仅针对用户，在推荐场景中对产品同样需要进行画像，这样形成的结果不光可以用来解释给一个特定的客户要推荐那些产品，反过来也可以解释给一个特定的产品要推荐给哪些用户。")]),s._v(" "),a("p",[s._v("​\t"),a("strong",[s._v("数据来源：")])]),s._v(" "),a("p",[s._v("​\t前面提到了数据如何处理的问题，他需要从业务系统中整理出来。 但是很多场景下，难的不是如何去清洗数据，而是压根就没有历史数据。这也就是所谓的冷启动问题。比如新上一个电影推荐系统，还完全没有历史业务数据，那要怎么开展基于机器学习的电影推荐呢？")]),s._v(" "),a("p",[s._v("这个时候，往往会采取一些措施来人工产生一些数据。比如像豆瓣上对电影的评价，就是一些比价好的数据。这时有两种大的渠道。一种是邀请专家来评分，比如邀请专业影评人来进行评分。这种数据称为PGC( Professionally-generated Content )：专家数据。另一种就是广邀群众来进行评分。这种数据称为UGC(  User-generated Content  )：用户数据。比如豆瓣上的评分就是这种类型。这两种数据，在进行数据处理前，其实是可以分开进行处理的。往往PGC数据质量比较高，而UGC的数据质量则相对较低。这时，是可以对这些数据做不同处理的。例如对UGC，最好是去掉一些明显比较偏颇的数据。这样，整体的数据质量更高，机器学习的上限也会更高。另外，在评分的同时，往往也需要综合他们的评论。而对评论这样的文本数据要怎么进行特征抽取呢？这就是我们之前提到的特征工程了。")]),s._v(" "),a("h3",{attrs:{id:"_2、推荐算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、推荐算法"}},[s._v("#")]),s._v(" 2、推荐算法")]),s._v(" "),a("p",[s._v("​\t前面说了一些前置的数据处理问题，接下来进入推荐算法的正题。")]),s._v(" "),a("p",[a("strong",[s._v("协同过滤 *"),a("em",[s._v("Collaborative Filtering Recommendation*")]),s._v("：")])]),s._v(" "),a("p",[s._v("​\t我们之前提到，推荐算法的本质就是对推荐矩阵里的缺失值进行填充的过程。那要怎么填充才会比较合理呢？这就涉及到推荐算法的根基：协同过滤( "),a("strong",[s._v("Collaborative Filtering Recommendation")]),s._v(")。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624180356919.png",alt:"image-20220624180356919"}})]),s._v(" "),a("p",[s._v("​\t他的基础思想就是先使用一些统计学的方法，寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。例如在上图中，用户a和用户c，有比较相同的喜好，都喜欢物品A和物品C。那这样，用户c对物品D的喜好就比较适合推荐给用户a。")]),s._v(" "),a("p",[s._v("这种方式，就是以用户为基础的协同过滤，称为UCF(User-based  Collaborative  Filtering)。其实反过来，从物品的角度，也可以来寻找对应的用户，建立推荐关系。这就称为CCF(Content-based  Collaborative Filtering)。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624180404681.png",alt:"image-20220624180404681"}})]),s._v(" "),a("p",[s._v("比如我们看这样的一个推荐矩阵。我需要预测用户C对于机器总动员这部电影的喜爱程度。喜爱程度较高，就需要给用户C主动发送机器总动员电影上映的推荐信息。这个时候要怎么去填充?位置的值呢？首先从用户的角度，横向的来看，用户A与用户C有很多相似的爱好。而对于机器总动员这部电影，用户A的评价很高。那我们就有理由去预测，用户C对机器总动员电影的喜爱程度也会比较高，给他填个4或者5都是比较合理的，这样对用户C，就可以有针对性的主动发送机器总动员这部电影的推荐信息了。这就是基于UCF的一种推荐机制。另外，从电影的角度，纵向来来，多啦A梦这部电影与机器总动员这部电影在用户群里中的评价也很相似，而对于多啦A梦这部电影，用户C的评价比较高，那同样我们也有理由去预测，用户C对机器总动员电影的喜爱程度会比较高，同样给他填个4或者5都是比较合理的。这样如果电影院要推广机器总动员这部电影，就可以主动向用户C发送推荐信息。这就是基于CCF的一种推荐机制。")]),s._v(" "),a("p",[a("strong",[s._v("隐语义模型： Latent Factor Model")])]),s._v(" "),a("p",[s._v("​\t有了协同过滤的这个基础的机制，那剩下就是如何去实现了。前面我们只是简单的通过直观判断分析出矩阵之中的一些相似性。但是这种相似性毕竟太过感性化。基于这种机制，通常可以再转为使用KNN算法，以基于K个邻居的历史偏好信息，为当前用户进行推荐。")]),s._v(" "),a("p",[s._v("​\t但是，转为使用KNN后，有什么不好呢？KNN是基于特征值+目标值的数据结构，那整个数据的结构就需要调整。所以对于已经准备好的推荐矩阵的挖掘理解实际上是不够的。那有没有一种更好的算法实现方式呢？这就可以考虑隐语义模型。")]),s._v(" "),a("p",[s._v("​\t隐语义模型的基础思想是这样的，如果可以找到两个矩阵，其中一个矩阵与推荐矩阵有相同的行(用户)，另一个矩阵与推荐矩阵有相同的列(产品)，然后他们两个矩阵的乘积正好满足推荐矩阵。那就可以用这两个矩阵来计算出完整的推荐矩阵。")]),s._v(" "),a("p",[s._v("​\t这样就把一个充满玄学的感性问题，转换成了一个数学求解的问题了。这个求解过程是比较复杂的，我们这里就不再去讨论这样的数据问题了。但是我们从工程上理解一下，如果真的找到了这样的两个矩阵，实际上就代表我们找到了一种UCF与CCF之间的关联。")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://404500.oss-cn-beijing.aliyuncs.com/picture/springboot/image-20220624180423821.png",alt:"image-20220624180423821"}})]),s._v(" "),a("p",[s._v('例如通过这个示例，我们通过一个不完整的用户评分矩阵R\'，找到了一个用户特征矩阵P和电影特征矩阵Q，  P*Q能够满足已有的评分矩阵R\'，并且他们两个矩阵相乘后，就得到了一个新的完整的用户评分矩阵R。我们可以认为有一些隐藏的因素，影响了用户对电影的评分。这其中f1,f2,f3,f4就代表了某种潜在的关联特征，比如电影的演员、题材、年代或者是一些人所不能理解的隐藏因子。这种感觉就很像我们经常提到的"直觉"或者"玄学"。这也就是我们所说的隐语义模型。但是事实上，我们并不需要真正去理解这些隐藏因子是什么，通过这些隐藏因子，就可以找到用户与电影的关联关系，就可以推测用户是否会喜欢某一部没有看过的电影。')]),s._v(" "),a("p",[s._v("接下来，应该要知道，这样的用户特征矩阵和电影特征矩阵应该是不止一个解的，所以，还需要有一个算法来对计算的结果进行评测，寻找一组最优化的解。并且，在机器学习中，由于有过拟合问题的存在，最优解一般来说并不是最正确的解。有一些小的误差，往往更能适合对实际问题进行预测。")]),s._v(" "),a("p",[s._v("这个过程同样会有几种方法，而在Spark中我们集成了其中一种方法。 最小二乘法：ALS( Alternating Least Square  :最小交替二乘法)。他的基础思想是对P,Q两个矩阵，先固定其中第一个矩阵，去求第二个矩阵的最优解，接下来再固定第二个矩阵，转过来求第一个矩阵的最优解。通过多次交替迭代，最终得到最佳的P,Q组合解。这是一种梯度下降，逼近最优解的过程。")]),s._v(" "),a("h3",{attrs:{id:"_3、算法实战"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、算法实战"}},[s._v("#")]),s._v(" 3、算法实战")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("Spark Demo:\n基于DF的ml实现\n1、org.apache.spark.examples.ml.JavaALSExample\n基于RDD的mllib实现\n1、org.apache.spark.examples.mllib.JavaRankingMetricsExample\n2、org.apache.spark.examples.mllib.JavaRecommendationExample\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br")])]),a("p",[s._v("其中，Spark推荐的是基于RDD的mllib实现。其中的核心代码就是创建ALS的过程：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("MatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(ratings), rank, numIterations, 0.01);\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("对于ALS.train方法，第一个参数就是推荐向量集合。第二个参数rank就是隐语义的个数。第三个参数就是交替执行的次数。第四个参数是正则化系数，你可以认为是用来优化结果的一个参数。")]),s._v(" "),a("p",[s._v("然后，构建出来的这个model就是训练得到的一个推荐模型。通过这个推荐模型，就很容易构建出完整的推荐系统。")]),s._v(" "),a("div",{staticClass:"language-java line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//拿到所有用户的推荐产品ID。也就是UCF的结果")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("RDD")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Tuple2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("double")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),s._v(" userFeatures "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("userFeatures")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//拿到所有产品的推荐用户ID，也就是CCF的结果")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("RDD")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Tuple2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("double")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),s._v(" productFeatures "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("productFeatures")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//使用模型来预测用户与产品之间的推荐分数。预测分数与推荐分数是会存在差异的，他们之间的均方差等指标也是评价模型的一个指标。")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("RDD")]),a("span",{pre:!0,attrs:{class:"token generics"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Rating")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(">")])]),s._v(" predictRDD "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("predict")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("JavaRDD")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("toRDD")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("userProducts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//给所有用户推荐10个产品。 包含原始矩阵中的记录")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("RDD")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Tuple2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Rating")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),s._v(" productsForUsers "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("recommendProductsForUsers")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//给所有产品推荐10个用户。 同样包含原始矩阵中的记录")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("RDD")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Tuple2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Object")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("Rating")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">>")]),s._v(" usersForProducts "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("recommendUsersForProducts")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("h3",{attrs:{id:"_4、结果评估与模型优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、结果评估与模型优化"}},[s._v("#")]),s._v(" 4、结果评估与模型优化")]),s._v(" "),a("p",[s._v("​\t从Spark提供的示例当中，应该要注意到，对于推荐矩阵的求解问题，实现的API是非常简单的。但是，示例中有很大一部分的工作都是在从各个角度对推荐矩阵训练出来的模型进行评估。")]),s._v(" "),a("p",[s._v("​\t所以，我们在构建一个推荐系统时，对于最核心的推荐算法，往往需要通过调整不同的超参数(例如推荐系统中的隐语义个数、迭代次数、正则化参数)，形成多个推荐模型。然后对这些推荐模型进行多个角度的综合排名。然后选取其中排名比较靠前的多个推荐算法，作为候选算法。")]),s._v(" "),a("p",[s._v("​\t接下来需要在后面的实际业务中，不断观察这些模型的真实预测表现，对这些模型的业务效果进行衡量。而衡量推荐系统效果的方式，主要有三种")]),s._v(" "),a("p",[s._v("1、基于常识的评判标准。")]),s._v(" "),a("p",[s._v("2、基于业务的评判标准。")]),s._v(" "),a("p",[s._v("3、基于机器学习的评判标准。")]),s._v(" "),a("p",[s._v("这些在最开始介绍推荐系统时已经给大家介绍过。其中第一点和第二点，还是比较好理解的。但是对于第三点，在开始的时候并没有说得太清楚。而现在，我们看到Spark示例当中的这些指标都可以作为实际的评判标准。")]),s._v(" "),a("p",[s._v("​\t最后，推荐系统的模型并不是一成不变的，并不是说训练出了一个最好的模型，那这个模型就永远是最好的了。所以，在后续业务过程中，还需要结合新的业务数据，对模型继续进行计算、评估、优化这样的过程。")]),s._v(" "),a("h3",{attrs:{id:"总结-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结-3"}},[s._v("#")]),s._v(" 总结")]),s._v(" "),a("p",[s._v("到这里，我们整个课程就告一段落。  在这个课程中，我们提到了推荐算法的一种实现，ALS。并且在推荐系统之外，还带大家了解了机器学习常用的一些方法、工具以及算法。通过这几节课程，希望大家能够对机器学习有一个初步的理解，对于机器学习中常用的这些术语有一个基础的理解，并且能够使用简单的API来解决一些基础的机器学习问题。其实大家也会了解到，对于机器学习领域，工程化实现往往并不难，而且往往随意用机器学习算法一尝试，就能达到一个不错的效果。而难的是对算法的选择以及对模型的调优。")]),s._v(" "),a("p",[s._v("当然，我们这几节课是尽量绕过了复杂的理论知识，只是从工程实现的角度带大家完成快速的入门，还是有很多不足的。一方面，整个推荐系统实施的工程基础还是比较薄弱的。像Hadoop、Spark这些典型的大数据组件还没有了解。另一方面，如果要真正深入把握机器学习的这些方法，这些简单的API背后的理论知识，各种算法的原理，还是绕不开的。最后，在传统机器学习的基础上，还有更强大的基于神经网络的深度学习方法也还没有接触。所以，对于人工智能这个领域，我们还真的只是在门口看了看热闹。在后续的课程中，将会逐步带大家继续深入接触人工智能的精彩")])])}),[],!1,null,null,null);a.default=n.exports}}]);