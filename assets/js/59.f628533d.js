(window.webpackJsonp=window.webpackJsonp||[]).push([[59],{403:function(s,a,e){"use strict";e.r(a);var n=e(1),r=Object(n.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("p",[s._v("Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协\n调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系\n统、低延迟的实时系统、Storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，\nLinkedin于2010年贡献给了Apache基金会并成为顶级开源 项目。")]),s._v(" "),a("h2",{attrs:{id:"kafka的使用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka的使用场景"}},[s._v("#")]),s._v(" Kafka的使用场景")]),s._v(" "),a("ul",[a("li",[s._v("日志收集：一个公司可以用Kafka收集各种服务的log，通过kafka以统一接口服务的方式开放给各种\nconsumer，例如hadoop、Hbase、Solr等。")]),s._v(" "),a("li",[s._v("消息系统：解耦和生产者和消费者、缓存消息等。\n用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这\n些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到\nhadoop、数据仓库中做离线分析和挖掘。")]),s._v(" "),a("li",[s._v("运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反\n馈，比如报警和报告。")])]),s._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/20.png",alt:""}})]),s._v(" "),a("h2",{attrs:{id:"kafka基本概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka基本概念"}},[s._v("#")]),s._v(" Kafka基本概念")]),s._v(" "),a("p",[s._v("kafka是一个分布式的，分区的消息(官方称之为commit log)服务。它提供一个消息系统应该具备的功能，但是确有着独\n特的设计。可以这样来说，Kafka借鉴了JMS规范的思想，但是确并没有完全遵循JMS规范。\n首先，让我们来看一下基础的消息(Message)相关术语：\n"),a("img",{attrs:{src:"C:%5CUsers%5CNO%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210902224735563.png",alt:"image-20210902224735563"}}),s._v("\n因此，从一个较高的层面上来看，producer通过网络发送消息到Kafka集群，然后consumer来进行消费，如下图：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/21.jpg",alt:""}})]),s._v(" "),a("p",[s._v("服务端(brokers)和客户端(producer、consumer)之间通信通过TCP协议来完成。")]),s._v(" "),a("h2",{attrs:{id:"kafka基本使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka基本使用"}},[s._v("#")]),s._v(" kafka基本使用")]),s._v(" "),a("p",[a("strong",[s._v("安装前的环境准备")]),s._v("\n由于Kafka是用Scala语言开发的，运行在JVM上，因此在安装Kafka之前需要先安装JDK。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("yum install java-1.8.0-openjdk* -y\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("kafka依赖zookeeper，所以需要先安装zookeeper")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("wget https://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.5.8/apache-zookeeper-3.5.8-bin.tar.gz\ntar -zxvf apache-zookeeper-3.5.8-bin.tar.gz\ncd  apache-zookeeper-3.5.8-bin\ncp conf/zoo_sample.cfg conf/zoo.cfg\n\n# 启动zookeeper\nbin/zkServer.sh start\nbin/zkCli.sh \nls /\t\t\t#查看zk的根目录相关节点\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("p",[s._v("第一步：下载安装包\n下载2.4.1 release版本，并解压：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("wget https://mirror.bit.edu.cn/apache/kafka/2.4.1/kafka_2.11-2.4.1.tgz  # 2.11是scala的版本，2.4.1是kafka的版本\ntar -xzf kafka_2.11-2.4.1.tgz\ncd kafka_2.11-2.4.1\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("第二步：修改配置\n修改配置文件config/server.properties:")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("#broker.id属性在kafka集群中必须要是唯一\nbroker.id=0\n#kafka部署的机器ip和提供服务的端口号\nlisteners=PLAINTEXT://192.168.65.60:9092   \n#kafka的消息存储文件\nlog.dir=/usr/local/data/kafka-logs\n#kafka连接zookeeper的地址\nzookeeper.connect=192.168.65.60:2181\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br")])]),a("p",[s._v("第三步：启动服务\n现在来启动kafka服务：\n启动脚本语法："),a("code",[s._v("kafka­server­start.sh [­daemon] server.properties")]),s._v("\n可以看到，"),a("code",[s._v("server.properties")]),s._v("的配置路径是一个强制的参数，"),a("code",[s._v("­daemon")]),s._v("表示以后台进程运行，否则ssh客户端退出后，\n就会停止服务。(注意，在启动kafka时会使用linux主机名关联的ip地址，所以需要把主机名和linux的ip映射配置到本地\nhost里，用vim /etc/hosts)")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# 启动kafka，运行日志在logs目录的server.log文件里\nbin/kafka-server-start.sh -daemon config/server.properties   #后台启动，不会打印日志到控制台\n或者用\nbin/kafka-server-start.sh config/server.properties &\n\n# 我们进入zookeeper目录通过zookeeper客户端查看下zookeeper的目录树\nbin/zkCli.sh \nls /\t\t#查看zk的根目录kafka相关节点\nls /brokers/ids\t#查看kafka节点\n\n# 停止kafka\nbin/kafka-server-stop.sh\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("p",[s._v("server.properties核心配置详解：")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[a("strong",[s._v("Property")])]),s._v(" "),a("th",[a("strong",[s._v("Default")])]),s._v(" "),a("th",[a("strong",[s._v("Description")])])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("broker.id")]),s._v(" "),a("td",[s._v("0")]),s._v(" "),a("td",[s._v("每个broker都可以用一个唯一的非负整数id进行标识；这个id可以作为broker的“名字”，你可以选择任意你喜欢的数字作为id，只要id是唯一的即可。")])]),s._v(" "),a("tr",[a("td",[s._v("log.dirs")]),s._v(" "),a("td",[s._v("/tmp/kafka-logs")]),s._v(" "),a("td",[s._v("kafka存放数据的路径。这个路径并不是唯一的，可以是多个，路径之间只需要使用逗号分隔即可；每当创建新partition时，都会选择在包含最少partitions的路径下进行。")])]),s._v(" "),a("tr",[a("td",[s._v("listeners")]),s._v(" "),a("td",[s._v("PLAINTEXT://192.168.65.60:9092")]),s._v(" "),a("td",[s._v("server接受客户端连接的端口，ip配置kafka本机ip即可")])]),s._v(" "),a("tr",[a("td",[s._v("zookeeper.connect")]),s._v(" "),a("td",[s._v("localhost:2181")]),s._v(" "),a("td",[s._v("zooKeeper连接字符串的格式为：hostname:port，此处hostname和port分别是ZooKeeper集群中某个节点的host和port；zookeeper如果是集群，连接方式为 hostname1:port1, hostname2:port2, hostname3:port3")])]),s._v(" "),a("tr",[a("td",[s._v("log.retention.hours")]),s._v(" "),a("td",[s._v("168")]),s._v(" "),a("td",[s._v("每个日志文件删除之前保存的时间。默认数据保存时间对所有topic都一样。")])]),s._v(" "),a("tr",[a("td",[s._v("num.partitions")]),s._v(" "),a("td",[s._v("1")]),s._v(" "),a("td",[s._v("创建topic的默认分区数")])]),s._v(" "),a("tr",[a("td",[s._v("default.replication.factor")]),s._v(" "),a("td",[s._v("1")]),s._v(" "),a("td",[s._v("自动创建topic的默认副本数量，建议设置为大于等于2")])]),s._v(" "),a("tr",[a("td",[s._v("min.insync.replicas")]),s._v(" "),a("td",[s._v("1")]),s._v(" "),a("td",[s._v("当producer设置acks为-1时，min.insync.replicas指定replicas的最小数目（必须确认每一个repica的写数据都是成功的），如果这个数目没有达到，producer发送消息会产生异常")])]),s._v(" "),a("tr",[a("td",[s._v("delete.topic.enable")]),s._v(" "),a("td",[s._v("false")]),s._v(" "),a("td",[s._v("是否允许删除主题")])])])]),s._v(" "),a("p",[s._v("第四步：创建主题\n现在我们来创建一个名字为“test”的Topic，这个topic只有一个partition，并且备份因子也设置为1：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --create --zookeeper 192.168.65.60:2181 --replication-factor 1 --partitions 1 --topic test\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("现在我们可以通过以下命令来查看kafka中目前存在的topic")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --list --zookeeper 192.168.65.60:2181\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("除了我们通过手工的方式创建Topic，当producer发布一个消息到某个指定的Topic，这个Topic如果不存在，就自动创\n建。\n删除主题")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --delete --topic test --zookeeper 192.168.65.60:2181\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("第五步：发送消息\nkafka自带了一个producer命令客户端，可以从本地文件中读取内容，或者我们也可以以命令行中直接输入内容，并将这\n些内容以消息的形式发送到kafka集群中。在默认情况下，每一个行会被当做成一个独立的消息。\n首先我们要运行发布消息的脚本，然后在命令中输入要发送的消息的内容：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-producer.sh --broker-list 192.168.65.60:9092 --topic test \n>this is a msg\n>this is a another msg \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("第六步：消费消息\n对于consumer，kafka同样也携带了一个命令行客户端，会将获取到内容在命令中进行输出，")]),s._v(" "),a("p",[s._v("默认是消费最新的消息：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092 --topic test   \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("如果想要消费之前的消息可以通过--from-beginning参数指定，如下命令：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092 --from-beginning --topic test \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("如果你是通过不同的终端窗口来运行以上的命令，你将会看到在producer终端输入的内容，很快就会在consumer的终\n端窗口上显示出来。\n以上所有的命令都有一些附加的选项；当我们不携带任何参数运行命令的时候，将会显示出这个命令的详细用法。\n消费多主题")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092 --whitelist "test|test-2"\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("单播消费\n一条消息只能被某一个消费者消费的模式，类似queue模式，只需让所有消费者在同一个消费组里即可\n分别在两个客户端执行如下消费命令，然后往主题里发送消息，结果只有一个客户端能收到消息")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092  --consumer-property group.id=testGroup --topic test \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("多播消费\n一条消息能被多个消费者消费的模式，类似publish-subscribe模式费，针对Kafka同一条消息只能被同一个消费组下的某一个消\n费者消费的特性，要实现多播只要保证这些消费者属于不同的消费组即可。我们再增加一个消费者，该消费者属于testGroup-2消费\n组，结果两个客户端都能收到消息")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092 --consumer-property group.id=testGroup-2 --topic test \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("查看消费组名")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-consumer-groups.sh --bootstrap-server 192.168.65.60:9092 --list \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("查看消费组的消费偏移量")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-consumer-groups.sh --bootstrap-server 192.168.65.60:9092 --describe --group testGroup\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/22.png",alt:""}})]),s._v(" "),a("p",[s._v("**current-offset：**当前消费组的已消费偏移量")]),s._v(" "),a("p",[s._v("**log-end-offset：**主题对应分区消息的结束偏移量(HW)")]),s._v(" "),a("p",[s._v("**lag：**当前消费组未消费的消息数")]),s._v(" "),a("p",[a("strong",[s._v("主题Topic和消息日志Log")])]),s._v(" "),a("hr"),s._v(" "),a("p",[a("strong",[s._v("可以理解Topic是一个类别的名称，同类消息发送到同一个Topic下面。对于每一个Topic，下面可以有多个分区(Partition)日志文件:")])]),s._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/23.png",alt:""}})]),s._v(" "),a("p",[s._v("Partition是一个"),a("strong",[s._v("有序的message序列")]),s._v("，这些message按顺序添加到一个叫做"),a("strong",[s._v("commit log的文件")]),s._v("中。每个partition中的消息都有一个唯一的编号，称之为offset，用来唯一标示某个分区中的message。")]),s._v(" "),a("p",[a("strong",[s._v("每个partition，都对应一个commit log文件")]),s._v("。一个partition中的message的offset都是唯一的，但是不同的partition中的message的offset可能是相同的。")]),s._v(" "),a("p",[s._v("kafka一般不会删除消息，不管这些消息有没有被消费。只会根据配置的日志保留时间(log.retention.hours)确认消息多久被删除，默认保留最近一周的日志消息。kafka的性能与保留的消息数据量大小没有关系，因此保存大量的数据消息日志信息不会有什么影响。")]),s._v(" "),a("p",[a("strong",[s._v("每个consumer是基于自己在commit log中的消费进度(offset)来进行工作的")]),s._v("。在kafka中，"),a("strong",[s._v("消费offset由consumer自己来维护")]),s._v("；一般情况下我们按照顺序逐条消费commit log中的消息，当然我可以通过指定offset来重复消费某些消息，或者跳过某些消息。")]),s._v(" "),a("p",[s._v("这意味kafka中的consumer对集群的影响是非常小的，添加一个或者减少一个consumer，对于集群或者其他consumer来说，都是没有影响的，因为每个consumer维护各自的消费offset。")]),s._v(" "),a("p",[a("strong",[s._v("创建多个分区的主题：")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --create --zookeeper 192.168.65.60:2181 --replication-factor 1 --partitions 2 --topic test1\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("查看下topic的情况")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v(" bin/kafka-topics.sh --describe --zookeeper 192.168.65.60:2181 --topic test1\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/24.png",alt:""}})]),s._v(" "),a("p",[s._v("以下是输出内容的解释，第一行是所有分区的概要信息，之后的每一行表示每一个partition的信息。")]),s._v(" "),a("ul",[a("li",[s._v("leader节点负责给定partition的所有读写请求。")]),s._v(" "),a("li",[s._v("replicas 表示某个partition在哪几个broker上存在备份。不管这个几点是不是”leader“，甚至这个节点挂了，也会列出。")]),s._v(" "),a("li",[s._v("isr 是replicas的一个子集，它只列出当前还存活着的，并且"),a("strong",[s._v("已同步备份")]),s._v("了该partition的节点。")])]),s._v(" "),a("p",[s._v("我们可以运行相同的命令查看之前创建的名称为”test“的topic")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --describe --zookeeper 192.168.65.60:2181 --topic test \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/30.png",alt:""}})]),s._v(" "),a("p",[s._v("之前设置了topic的partition数量为1，备份因子为1，因此显示就如上所示了。")]),s._v(" "),a("p",[s._v("可以进入kafka的数据文件存储目录查看test和test1主题的消息日志文件：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/31.png",alt:""}})]),s._v(" "),a("p",[s._v("消息日志文件主要存放在分区文件夹里的以log结尾的日志文件里，如下是test1主题对应的分区0的消息日志：")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/32.png",alt:""}})]),s._v(" "),a("p",[s._v("当然我们也可以通过如下命令"),a("strong",[s._v("增加topic的分区数量(目前kafka不支持减少分区)")]),s._v("：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh -alter --partitions 3 --zookeeper 192.168.65.60:2181 --topic test\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("strong",[s._v("可以这么来理解Topic，Partition和Broker")])]),s._v(" "),a("p",[s._v("一个topic，代表逻辑上的一个业务数据集，比如按数据库里不同表的数据操作消息区分放入不同topic，订单相关操作消息放入订单topic，用户相关操作消息放入用户topic，对于大型网站来说，后端数据都是海量的，订单消息很可能是非常巨量的，比如有几百个G甚至达到TB级别，如果把这么多数据都放在一台机器上可定会有容量限制问题，那么就可以在topic内部划分多个partition来分片存储数据，不同的partition可以位于不同的机器上，每台机器上都运行一个Kafka的进程Broker。")]),s._v(" "),a("p",[a("strong",[s._v("为什么要对Topic下数据进行分区存储？")])]),s._v(" "),a("p",[s._v("1、commit log文件会受到所在机器的文件系统大小的限制，分区之后可以将不同的分区放在不同的机器上，相当于对数据做了"),a("strong",[s._v("分布式存储")]),s._v("，理论上一个topic可以处理任意数量的数据。")]),s._v(" "),a("p",[s._v("2、为了"),a("strong",[s._v("提高并行度")]),s._v("。")]),s._v(" "),a("h2",{attrs:{id:"kafka集群实战"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka集群实战"}},[s._v("#")]),s._v(" kafka集群实战")]),s._v(" "),a("hr"),s._v(" "),a("p",[s._v("对于kafka来说，一个单独的broker意味着kafka集群中只有一个节点。要想增加kafka集群中的节点数量，只需要多启动几个broker实例即可。为了有更好的理解，现在我们在一台机器上同时启动三个broker实例。")]),s._v(" "),a("p",[s._v("首先，我们需要建立好其他2个broker的配置文件：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("cp config/server.properties config/server-1.properties\ncp config/server.properties config/server-2.properties\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("配置文件的需要修改的内容分别如下：")]),s._v(" "),a("p",[s._v("config/server-1.properties:")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("#broker.id属性在kafka集群中必须要是唯一\nbroker.id=1\n#kafka部署的机器ip和提供服务的端口号\nlisteners=PLAINTEXT://192.168.65.60:9093   \nlog.dir=/usr/local/data/kafka-logs-1\n#kafka连接zookeeper的地址，要把多个kafka实例组成集群，对应连接的zookeeper必须相同\nzookeeper.connect=192.168.65.60:2181\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("config/server-2.properties:")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("broker.id=2\nlisteners=PLAINTEXT://192.168.65.60:9094\nlog.dir=/usr/local/data/kafka-logs-2\nzookeeper.connect=192.168.65.60:2181\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("目前我们已经有一个zookeeper实例和一个broker实例在运行了，现在我们只需要在启动2个broker实例即可：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-server-start.sh -daemon config/server-1.properties\nbin/kafka-server-start.sh -daemon config/server-2.properties\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[a("strong",[s._v("查看zookeeper确认集群节点是否都注册成功：")])]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/40.png",alt:""}})]),s._v(" "),a("p",[s._v("现在我们创建一个新的topic，副本数设置为3，分区数设置为2：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --create --zookeeper 192.168.65.60:2181 --replication-factor 3 --partitions 2 --topic my-replicated-topic\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[a("strong",[s._v("查看下topic的情况")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v(" bin/kafka-topics.sh --describe --zookeeper 192.168.65.60:2181 --topic my-replicated-topic\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/41.png",alt:""}})]),s._v(" "),a("p",[s._v("以下是输出内容的解释，第一行是所有分区的概要信息，之后的每一行表示每一个partition的信息。")]),s._v(" "),a("ul",[a("li",[s._v("leader节点负责给定partition的所有读写请求，同一个主题不同分区leader副本一般不一样(为了容灾)")]),s._v(" "),a("li",[s._v("replicas 表示某个partition在哪几个broker上存在备份。不管这个几点是不是”leader“，甚至这个节点挂了，也会列出。")]),s._v(" "),a("li",[s._v("isr 是replicas的一个子集，它只列出当前还存活着的，并且"),a("strong",[s._v("已同步备份")]),s._v("了该partition的节点。")])]),s._v(" "),a("p",[s._v("现在我们向新建的 my-replicated-topic 中发送一些message，kafka集群可以加上所有kafka节点：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-producer.sh --broker-list 192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094 --topic my-replicated-topic\n>my test msg 1\n>my test msg 2\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("现在开始消费：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094 --from-beginning --topic my-replicated-topic\nmy test msg 1\nmy test msg 2\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("现在我们来测试我们容错性，因为broker1目前是my-replicated-topic的分区0的leader，所以我们要将其kill")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("ps -ef | grep server.properties\nkill 14776\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("现在再执行命令：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-topics.sh --describe --zookeeper 192.168.65.60:9092 --topic my-replicated-topic\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/42.png",alt:""}})]),s._v(" "),a("p",[s._v("我们可以看到，分区0的leader节点已经变成了broker 0。要注意的是，在Isr中，已经没有了1号节点。leader的选举也是从ISR(in-sync replica)中进行的。")]),s._v(" "),a("p",[s._v("此时，我们依然可以 消费新消息：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("bin/kafka-console-consumer.sh --bootstrap-server 192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094 --from-beginning --topic my-replicated-topic\nmy test msg 1\nmy test msg 2\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("查看主题分区对应的leader信息：")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/43.png",alt:""}})]),s._v(" "),a("p",[a("strong",[s._v("kafka将很多集群关键信息记录在zookeeper里，保证自己的无状态，从而在水平扩容时非常方便。")])]),s._v(" "),a("p",[a("strong",[s._v("集群消费")])]),s._v(" "),a("hr"),s._v(" "),a("p",[s._v("log的partitions分布在kafka集群中不同的broker上，每个broker可以请求备份其他broker上partition上的数据。kafka集群支持配置一个partition备份的数量。")]),s._v(" "),a("p",[s._v("针对每个partition，都有一个broker起到“leader”的作用，0个或多个其他的broker作为“follwers”的作用。"),a("strong",[s._v("leader处理所有的针对这个partition的读写请求，而followers被动复制leader的结果，不提供读写(主要是为了保证多副本数据与消费的一致性)")]),s._v("。如果这个leader失效了，其中的一个follower将会自动的变成新的leader。")]),s._v(" "),a("p",[a("strong",[s._v("Producers")])]),s._v(" "),a("p",[s._v("生产者将消息发送到topic中去，同时负责选择将message发送到topic的哪一个partition中。通过round-robin做简单的负载均衡。也可以根据消息中的某一个关键字来进行区分。通常第二种方式使用的更多。")]),s._v(" "),a("p",[a("strong",[s._v("Consumers")])]),s._v(" "),a("p",[s._v("传统的消息传递模式有2种：队列( queue) 和（publish-subscribe）")]),s._v(" "),a("ul",[a("li",[s._v("queue模式：多个consumer从服务器中读取数据，消息只会到达一个consumer。")]),s._v(" "),a("li",[s._v("publish-subscribe模式：消息会被广播给所有的consumer。")])]),s._v(" "),a("p",[s._v("Kafka基于这2种模式提供了一种consumer的抽象概念：consumer group。")]),s._v(" "),a("ul",[a("li",[s._v("queue模式：所有的consumer都位于同一个consumer group 下。")]),s._v(" "),a("li",[s._v("publish-subscribe模式：所有的consumer都有着自己唯一的consumer group。")])]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/44.png",alt:""}})]),s._v(" "),a("p",[s._v("上图说明：由2个broker组成的kafka集群，某个主题总共有4个partition(P0-P3)，分别位于不同的broker上。这个集群由2个Consumer Group消费， A有2个consumer instances ，B有4个。")]),s._v(" "),a("p",[s._v("通常一个topic会有几个consumer group，每个consumer group都是一个逻辑上的订阅者（ logical subscriber ）。每个consumer group由多个consumer instance组成，从而达到可扩展和容灾的功能。")]),s._v(" "),a("p",[a("strong",[s._v("消费顺序")])]),s._v(" "),a("p",[s._v("一个partition同一个时刻在一个consumer group中只能有一个consumer instance在消费，从而保证消费顺序。")]),s._v(" "),a("p",[a("strong",[s._v("consumer group中的consumer instance的数量不能比一个Topic中的partition的数量多，否则，多出来的consumer消费不到消息。")])]),s._v(" "),a("p",[s._v("Kafka只在partition的范围内保证消息消费的局部顺序性，不能在同一个topic中的多个partition中保证总的消费顺序性。")]),s._v(" "),a("p",[s._v("如果有在总体上保证消费顺序的需求，那么我们可以通过将topic的partition数量设置为1，将consumer group中的consumer instance数量也设置为1，但是这样会影响性能，所以kafka的顺序消费很少用。")]),s._v(" "),a("p",[a("strong",[s._v("Java客户端访问Kafka")])]),s._v(" "),a("p",[s._v("引入maven依赖")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("<dependency>\n   <groupId>org.apache.kafka</groupId>\n   <artifactId>kafka-clients</artifactId>\n   <version>2.4.1</version>\n</dependency>\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("消息发送端代码")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('package com.tuling.kafka.kafkaDemo;\n\nimport com.alibaba.fastjson.JSON;\nimport org.apache.kafka.clients.producer.*;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\n\npublic class MsgProducer {\n    private final static String TOPIC_NAME = "my-replicated-topic";\n\n    public static void main(String[] args) throws InterruptedException, ExecutionException {\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094");\n         /*\n         发出消息持久化机制参数\n        （1）acks=0： 表示producer不需要等待任何broker确认收到消息的回复，就可以继续发送下一条消息。性能最高，但是最容易丢消息。\n        （2）acks=1： 至少要等待leader已经成功将数据写入本地log，但是不需要等待所有follower是否成功写入。就可以继续发送下一\n             条消息。这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。\n        （3）acks=-1或all： 需要等待 min.insync.replicas(默认为1，推荐配置大于等于2) 这个参数配置的副本个数都成功写入日志，这种策略会保证\n            只要有一个备份存活就不会丢失数据。这是最强的数据保证。一般除非是金融级别，或跟钱打交道的场景才会使用这种配置。\n         */\n        /*props.put(ProducerConfig.ACKS_CONFIG, "1");\n         *//*\n        发送失败会重试，默认重试间隔100ms，重试能保证消息发送的可靠性，但是也可能造成消息重复发送，比如网络抖动，所以需要在\n        接收者那边做好消息接收的幂等性处理\n        *//*\n        props.put(ProducerConfig.RETRIES_CONFIG, 3);\n        //重试间隔设置\n        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 300);\n        //设置发送消息的本地缓冲区，如果设置了该缓冲区，消息会先发送到本地缓冲区，可以提高消息发送性能，默认值是33554432，即32MB\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);\n        *//*\n        kafka本地线程会从缓冲区取数据，批量发送到broker，\n        设置批量发送消息的大小，默认值是16384，即16kb，就是说一个batch满了16kb就发送出去\n        *//*\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);\n        *//*\n        默认值是0，意思就是消息必须立即被发送，但这样会影响性能\n        一般设置10毫秒左右，就是说这个消息发送完后会进入本地的一个batch，如果10毫秒内，这个batch满了16kb就会随batch一起被发送出去\n        如果10毫秒内，batch没满，那么也必须把消息发送出去，不能让消息的发送延迟时间太长\n        *//*\n        props.put(ProducerConfig.LINGER_MS_CONFIG, 10);*/\n        //把发送的key从字符串序列化为字节数组\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        //把发送消息value从字符串序列化为字节数组\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n        Producer<String, String> producer = new KafkaProducer<String, String>(props);\n\n        int msgNum = 5;\n        final CountDownLatch countDownLatch = new CountDownLatch(msgNum);\n        for (int i = 1; i <= msgNum; i++) {\n            Order order = new Order(i, 100 + i, 1, 1000.00);\n            //指定发送分区\n            /*ProducerRecord<String, String> producerRecord = new ProducerRecord<String, String>(TOPIC_NAME\n                    , 0, order.getOrderId().toString(), JSON.toJSONString(order));*/\n            //未指定发送分区，具体发送的分区计算公式：hash(key)%partitionNum\n            ProducerRecord<String, String> producerRecord = new ProducerRecord<String, String>(TOPIC_NAME\n                    , order.getOrderId().toString(), JSON.toJSONString(order));\n\n            //等待消息发送成功的同步阻塞方法\n            /*RecordMetadata metadata = producer.send(producerRecord).get();\n            System.out.println("同步方式发送消息结果：" + "topic-" + metadata.topic() + "|partition-"\n                    + metadata.partition() + "|offset-" + metadata.offset());*/\n\n            //异步回调方式发送消息\n            producer.send(producerRecord, new Callback() {\n                public void onCompletion(RecordMetadata metadata, Exception exception) {\n                    if (exception != null) {\n                        System.err.println("发送消息失败：" + exception.getStackTrace());\n\n                    }\n                    if (metadata != null) {\n                        System.out.println("异步方式发送消息结果：" + "topic-" + metadata.topic() + "|partition-"\n                                + metadata.partition() + "|offset-" + metadata.offset());\n                    }\n                    countDownLatch.countDown();\n                }\n            });\n\n            //送积分 TODO\n\n        }\n\n        countDownLatch.await(5, TimeUnit.SECONDS);\n        producer.close();\n    }\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br"),a("span",{staticClass:"line-number"},[s._v("51")]),a("br"),a("span",{staticClass:"line-number"},[s._v("52")]),a("br"),a("span",{staticClass:"line-number"},[s._v("53")]),a("br"),a("span",{staticClass:"line-number"},[s._v("54")]),a("br"),a("span",{staticClass:"line-number"},[s._v("55")]),a("br"),a("span",{staticClass:"line-number"},[s._v("56")]),a("br"),a("span",{staticClass:"line-number"},[s._v("57")]),a("br"),a("span",{staticClass:"line-number"},[s._v("58")]),a("br"),a("span",{staticClass:"line-number"},[s._v("59")]),a("br"),a("span",{staticClass:"line-number"},[s._v("60")]),a("br"),a("span",{staticClass:"line-number"},[s._v("61")]),a("br"),a("span",{staticClass:"line-number"},[s._v("62")]),a("br"),a("span",{staticClass:"line-number"},[s._v("63")]),a("br"),a("span",{staticClass:"line-number"},[s._v("64")]),a("br"),a("span",{staticClass:"line-number"},[s._v("65")]),a("br"),a("span",{staticClass:"line-number"},[s._v("66")]),a("br"),a("span",{staticClass:"line-number"},[s._v("67")]),a("br"),a("span",{staticClass:"line-number"},[s._v("68")]),a("br"),a("span",{staticClass:"line-number"},[s._v("69")]),a("br"),a("span",{staticClass:"line-number"},[s._v("70")]),a("br"),a("span",{staticClass:"line-number"},[s._v("71")]),a("br"),a("span",{staticClass:"line-number"},[s._v("72")]),a("br"),a("span",{staticClass:"line-number"},[s._v("73")]),a("br"),a("span",{staticClass:"line-number"},[s._v("74")]),a("br"),a("span",{staticClass:"line-number"},[s._v("75")]),a("br"),a("span",{staticClass:"line-number"},[s._v("76")]),a("br"),a("span",{staticClass:"line-number"},[s._v("77")]),a("br"),a("span",{staticClass:"line-number"},[s._v("78")]),a("br"),a("span",{staticClass:"line-number"},[s._v("79")]),a("br"),a("span",{staticClass:"line-number"},[s._v("80")]),a("br"),a("span",{staticClass:"line-number"},[s._v("81")]),a("br"),a("span",{staticClass:"line-number"},[s._v("82")]),a("br"),a("span",{staticClass:"line-number"},[s._v("83")]),a("br"),a("span",{staticClass:"line-number"},[s._v("84")]),a("br"),a("span",{staticClass:"line-number"},[s._v("85")]),a("br"),a("span",{staticClass:"line-number"},[s._v("86")]),a("br"),a("span",{staticClass:"line-number"},[s._v("87")]),a("br"),a("span",{staticClass:"line-number"},[s._v("88")]),a("br"),a("span",{staticClass:"line-number"},[s._v("89")]),a("br"),a("span",{staticClass:"line-number"},[s._v("90")]),a("br"),a("span",{staticClass:"line-number"},[s._v("91")]),a("br"),a("span",{staticClass:"line-number"},[s._v("92")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("消息接收端代码")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('package com.tuling.kafka.kafkaDemo;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.common.serialization.StringDeserializer;\n\nimport java.time.Duration;\nimport java.util.Arrays;\nimport java.util.Properties;\n\npublic class MsgConsumer {\n    private final static String TOPIC_NAME = "my-replicated-topic";\n    private final static String CONSUMER_GROUP_NAME = "testGroup";\n\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094");\n        // 消费分组名\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_NAME);\n        // 是否自动提交offset，默认就是true\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");\n        // 自动提交offset的间隔时间\n        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");\n        //props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");\n        /*\n        当消费主题的是一个新的消费组，或者指定offset的消费方式，offset不存在，那么应该如何消费\n        latest(默认) ：只消费自己启动之后发送到主题的消息\n        earliest：第一次从头开始消费，以后按照消费offset记录继续消费，这个需要区别于consumer.seekToBeginning(每次都从头开始消费)\n        */\n        //props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");\n      /*\n      consumer给broker发送心跳的间隔时间，broker接收到心跳如果此时有rebalance发生会通过心跳响应将\n      rebalance方案下发给consumer，这个时间可以稍微短一点\n      */\n        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 1000);\n        /*\n        服务端broker多久感知不到一个consumer心跳就认为他故障了，会将其踢出消费组，\n        对应的Partition也会被重新分配给其他consumer，默认是10秒\n        */\n        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10 * 1000);\n        //一次poll最大拉取消息的条数，如果消费者处理速度很快，可以设置大点，如果处理速度一般，可以设置小点\n        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);\n        /*\n        如果两次poll操作间隔超过了这个时间，broker就会认为这个consumer处理能力太弱，\n        会将其踢出消费组，将分区分配给别的consumer消费\n        */\n        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 30 * 1000);\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n\n        consumer.subscribe(Arrays.asList(TOPIC_NAME));\n        // 消费指定分区\n        //consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));\n\n        //消息回溯消费\n        /*consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));\n        consumer.seekToBeginning(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));*/\n\n        //指定offset消费\n        /*consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));\n        consumer.seek(new TopicPartition(TOPIC_NAME, 0), 10);*/\n\n        //从指定时间点开始消费\n        /*List<PartitionInfo> topicPartitions = consumer.partitionsFor(TOPIC_NAME);\n        //从1小时前开始消费\n        long fetchDataTime = new Date().getTime() - 1000 * 60 * 60;\n        Map<TopicPartition, Long> map = new HashMap<>();\n        for (PartitionInfo par : topicPartitions) {\n            map.put(new TopicPartition(topicName, par.partition()), fetchDataTime);\n        }\n        Map<TopicPartition, OffsetAndTimestamp> parMap = consumer.offsetsForTimes(map);\n        for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : parMap.entrySet()) {\n            TopicPartition key = entry.getKey();\n            OffsetAndTimestamp value = entry.getValue();\n            if (key == null || value == null) continue;\n            Long offset = value.offset();\n            System.out.println("partition-" + key.partition() + "|offset-" + offset);\n            System.out.println();\n            //根据消费里的timestamp确定offset\n            if (value != null) {\n                consumer.assign(Arrays.asList(key));\n                consumer.seek(key, offset);\n            }\n        }*/\n\n        while (true) {\n            /*\n             * poll() API 是拉取消息的长轮询\n             */\n            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));\n            for (ConsumerRecord<String, String> record : records) {\n                System.out.printf("收到消息：partition = %d,offset = %d, key = %s, value = %s%n", record.partition(),\n                        record.offset(), record.key(), record.value());\n            }\n\n            /*if (records.count() > 0) {\n                // 手动同步提交offset，当前线程会阻塞直到offset提交成功\n                // 一般使用同步提交，因为提交之后一般也没有什么逻辑代码了\n                consumer.commitSync();\n\n                // 手动异步提交offset，当前线程提交offset不会阻塞，可以继续处理后面的程序逻辑\n                consumer.commitAsync(new OffsetCommitCallback() {\n                    @Override\n                    public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {\n                        if (exception != null) {\n                            System.err.println("Commit failed for " + offsets);\n                            System.err.println("Commit failed exception: " + exception.getStackTrace());\n                        }\n                    }\n                });\n\n            }*/\n        }\n    }\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br"),a("span",{staticClass:"line-number"},[s._v("48")]),a("br"),a("span",{staticClass:"line-number"},[s._v("49")]),a("br"),a("span",{staticClass:"line-number"},[s._v("50")]),a("br"),a("span",{staticClass:"line-number"},[s._v("51")]),a("br"),a("span",{staticClass:"line-number"},[s._v("52")]),a("br"),a("span",{staticClass:"line-number"},[s._v("53")]),a("br"),a("span",{staticClass:"line-number"},[s._v("54")]),a("br"),a("span",{staticClass:"line-number"},[s._v("55")]),a("br"),a("span",{staticClass:"line-number"},[s._v("56")]),a("br"),a("span",{staticClass:"line-number"},[s._v("57")]),a("br"),a("span",{staticClass:"line-number"},[s._v("58")]),a("br"),a("span",{staticClass:"line-number"},[s._v("59")]),a("br"),a("span",{staticClass:"line-number"},[s._v("60")]),a("br"),a("span",{staticClass:"line-number"},[s._v("61")]),a("br"),a("span",{staticClass:"line-number"},[s._v("62")]),a("br"),a("span",{staticClass:"line-number"},[s._v("63")]),a("br"),a("span",{staticClass:"line-number"},[s._v("64")]),a("br"),a("span",{staticClass:"line-number"},[s._v("65")]),a("br"),a("span",{staticClass:"line-number"},[s._v("66")]),a("br"),a("span",{staticClass:"line-number"},[s._v("67")]),a("br"),a("span",{staticClass:"line-number"},[s._v("68")]),a("br"),a("span",{staticClass:"line-number"},[s._v("69")]),a("br"),a("span",{staticClass:"line-number"},[s._v("70")]),a("br"),a("span",{staticClass:"line-number"},[s._v("71")]),a("br"),a("span",{staticClass:"line-number"},[s._v("72")]),a("br"),a("span",{staticClass:"line-number"},[s._v("73")]),a("br"),a("span",{staticClass:"line-number"},[s._v("74")]),a("br"),a("span",{staticClass:"line-number"},[s._v("75")]),a("br"),a("span",{staticClass:"line-number"},[s._v("76")]),a("br"),a("span",{staticClass:"line-number"},[s._v("77")]),a("br"),a("span",{staticClass:"line-number"},[s._v("78")]),a("br"),a("span",{staticClass:"line-number"},[s._v("79")]),a("br"),a("span",{staticClass:"line-number"},[s._v("80")]),a("br"),a("span",{staticClass:"line-number"},[s._v("81")]),a("br"),a("span",{staticClass:"line-number"},[s._v("82")]),a("br"),a("span",{staticClass:"line-number"},[s._v("83")]),a("br"),a("span",{staticClass:"line-number"},[s._v("84")]),a("br"),a("span",{staticClass:"line-number"},[s._v("85")]),a("br"),a("span",{staticClass:"line-number"},[s._v("86")]),a("br"),a("span",{staticClass:"line-number"},[s._v("87")]),a("br"),a("span",{staticClass:"line-number"},[s._v("88")]),a("br"),a("span",{staticClass:"line-number"},[s._v("89")]),a("br"),a("span",{staticClass:"line-number"},[s._v("90")]),a("br"),a("span",{staticClass:"line-number"},[s._v("91")]),a("br"),a("span",{staticClass:"line-number"},[s._v("92")]),a("br"),a("span",{staticClass:"line-number"},[s._v("93")]),a("br"),a("span",{staticClass:"line-number"},[s._v("94")]),a("br"),a("span",{staticClass:"line-number"},[s._v("95")]),a("br"),a("span",{staticClass:"line-number"},[s._v("96")]),a("br"),a("span",{staticClass:"line-number"},[s._v("97")]),a("br"),a("span",{staticClass:"line-number"},[s._v("98")]),a("br"),a("span",{staticClass:"line-number"},[s._v("99")]),a("br"),a("span",{staticClass:"line-number"},[s._v("100")]),a("br"),a("span",{staticClass:"line-number"},[s._v("101")]),a("br"),a("span",{staticClass:"line-number"},[s._v("102")]),a("br"),a("span",{staticClass:"line-number"},[s._v("103")]),a("br"),a("span",{staticClass:"line-number"},[s._v("104")]),a("br"),a("span",{staticClass:"line-number"},[s._v("105")]),a("br"),a("span",{staticClass:"line-number"},[s._v("106")]),a("br"),a("span",{staticClass:"line-number"},[s._v("107")]),a("br"),a("span",{staticClass:"line-number"},[s._v("108")]),a("br"),a("span",{staticClass:"line-number"},[s._v("109")]),a("br"),a("span",{staticClass:"line-number"},[s._v("110")]),a("br"),a("span",{staticClass:"line-number"},[s._v("111")]),a("br"),a("span",{staticClass:"line-number"},[s._v("112")]),a("br"),a("span",{staticClass:"line-number"},[s._v("113")]),a("br"),a("span",{staticClass:"line-number"},[s._v("114")]),a("br"),a("span",{staticClass:"line-number"},[s._v("115")]),a("br"),a("span",{staticClass:"line-number"},[s._v("116")]),a("br"),a("span",{staticClass:"line-number"},[s._v("117")]),a("br"),a("span",{staticClass:"line-number"},[s._v("118")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[a("strong",[s._v("Spring Boot整合Kafka")])]),s._v(" "),a("p",[s._v("引入spring boot kafka依赖，详见项目实例：spring-boot-kafka")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("<dependency>\n    <groupId>org.springframework.kafka</groupId>\n    <artifactId>spring-kafka</artifactId>\n</dependency>\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("application.yml配置如下：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("server:\n  port: 8080\n\nspring:\n  kafka:\n    bootstrap-servers: 192.168.65.60:9092,192.168.65.60:9093,192.168.65.60:9094\n    producer: # 生产者\n      retries: 3 # 设置大于0的值，则客户端会将发送失败的记录重新发送\n      batch-size: 16384\n      buffer-memory: 33554432\n      acks: 1\n      # 指定消息key和消息体的编解码方式\n      key-serializer: org.apache.kafka.common.serialization.StringSerializer\n      value-serializer: org.apache.kafka.common.serialization.StringSerializer\n    consumer:\n      group-id: default-group\n      enable-auto-commit: false\n      auto-offset-reset: earliest\n      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer\n      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer\n    listener:\n      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交\n      # RECORD\n      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交\n      # BATCH\n      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交\n      # TIME\n      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交\n      # COUNT\n      # TIME |　COUNT　有一个条件满足时提交\n      # COUNT_TIME\n      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交\n      # MANUAL\n      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种\n      # MANUAL_IMMEDIATE\n      ack-mode: manual_immediate\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("发送者代码：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('package com.kafka;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class KafkaController {\n\n    private final static String TOPIC_NAME = "my-replicated-topic";\n\n    @Autowired\n    private KafkaTemplate<String, String> kafkaTemplate;\n\n    @RequestMapping("/send")\n    public void send() {\n        kafkaTemplate.send(TOPIC_NAME, 0, "key", "this is a msg");\n    }\n\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("消费者代码：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('package com.kafka;\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.springframework.kafka.annotation.KafkaListener;\nimport org.springframework.kafka.support.Acknowledgment;\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class MyConsumer {\n\n    /**\n     * @KafkaListener(groupId = "testGroup", topicPartitions = {\n     *             @TopicPartition(topic = "topic1", partitions = {"0", "1"}),\n     *             @TopicPartition(topic = "topic2", partitions = "0",\n     *                     partitionOffsets = @PartitionOffset(partition = "1", initialOffset = "100"))\n     *     },concurrency = "6")\n     *  //concurrency就是同组下的消费者个数，就是并发消费数，必须小于等于分区总数\n     * @param record\n     */\n    @KafkaListener(topics = "my-replicated-topic",groupId = "zhugeGroup")\n    public void listenZhugeGroup(ConsumerRecord<String, String> record, Acknowledgment ack) {\n        String value = record.value();\n        System.out.println(value);\n        System.out.println(record);\n        //手动提交offset\n        ack.acknowledge();\n    }\n\n    /*//配置多个消费组\n    @KafkaListener(topics = "my-replicated-topic",groupId = "tulingGroup")\n    public void listenTulingGroup(ConsumerRecord<String, String> record, Acknowledgment ack) {\n        String value = record.value();\n        System.out.println(value);\n        System.out.println(record);\n        ack.acknowledge();\n    }*/\n}\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("h2",{attrs:{id:"kafka设计原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka设计原理"}},[s._v("#")]),s._v(" kafka设计原理")]),s._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/50.jpg",alt:""}})]),s._v(" "),a("h2",{attrs:{id:"kafka核心总控制器controller"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka核心总控制器controller"}},[s._v("#")]),s._v(" Kafka核心总控制器Controller")]),s._v(" "),a("p",[s._v("在Kafka集群中会有一个或者多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。")]),s._v(" "),a("ul",[a("li",[s._v("当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。")]),s._v(" "),a("li",[s._v("当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。")]),s._v(" "),a("li",[s._v("当使用kafka-topics.sh脚本为某个topic增加分区数量时，同样还是由控制器负责让新分区被其他节点感知到。")])]),s._v(" "),a("p",[a("strong",[s._v("Controller选举机制")])]),s._v(" "),a("p",[s._v("在kafka集群启动的时候，会自动选举一台broker作为controller来管理整个集群，选举的过程是集群中每个broker都会尝试在zookeeper上创建一个 "),a("strong",[s._v("/controller 临时节点")]),s._v("，zookeeper会保证有且仅有一个broker能创建成功，这个broker就会成为集群的总控器controller。")]),s._v(" "),a("p",[s._v("当这个controller角色的broker宕机了，此时zookeeper临时节点会消失，集群里其他broker会一直监听这个临时节点，发现临时节点消失了，就竞争再次创建临时节点，就是我们上面说的选举机制，zookeeper又会保证有一个broker成为新的controller。")]),s._v(" "),a("p",[s._v("具备控制器身份的broker需要比其他普通的broker多一份职责，具体细节如下：")]),s._v(" "),a("ol",[a("li",[a("strong",[s._v("监听broker相关的变化")]),s._v("。为Zookeeper中的/brokers/ids/节点添加BrokerChangeListener，用来处理broker增减的变化。")]),s._v(" "),a("li",[s._v("监听topic相关的变化。为Zookeeper中的/brokers/topics节点添加TopicChangeListener，用来处理topic增减的变化；为Zookeeper中的/admin/delete_topics节点添加TopicDeletionListener，用来处理删除topic的动作。")]),s._v(" "),a("li",[s._v("从Zookeeper中读取获取当前所有与topic、partition以及broker有关的信息并进行相应的管理。对于所有topic所对应的Zookeeper中的/brokers/topics/[topic]节点添加PartitionModificationsListener，用来监听topic中的分区分配变化。")]),s._v(" "),a("li",[s._v("更新集群的元数据信息，同步到其他普通的broker节点中。")])]),s._v(" "),a("p",[a("strong",[s._v("Partition副本选举Leader机制")])]),s._v(" "),a("p",[s._v("controller感知到分区leader所在的broker挂了(controller监听了很多zk节点可以感知到broker存活)，controller会从ISR列表(参数unclean.leader.election.enable=false的前提下)里挑第一个broker作为leader(第一个broker最先放进ISR列表，可能是同步数据最多的副本)，如果参数unclean.leader.election.enable为true，代表在ISR列表里所有副本都挂了的时候可以在ISR列表以外的副本中选leader，这种设置，可以提高可用性，但是选出的新leader有可能数据少很多。")]),s._v(" "),a("p",[s._v("副本进入ISR列表有两个条件：")]),s._v(" "),a("ol",[a("li",[s._v("副本节点不能产生分区，必须能与zookeeper保持会话以及跟leader副本网络连通")]),s._v(" "),a("li",[s._v("副本能复制leader上的所有写操作，并且不能落后太多。(与leader副本同步滞后的副本，是由 replica.lag.time.max.ms 配置决定的，超过这个时间都没有跟leader同步过的一次的副本会被移出ISR列表)")])]),s._v(" "),a("p",[a("strong",[s._v("消费者消费消息的offset记录机制")])]),s._v(" "),a("p",[s._v("每个consumer会定期将自己消费分区的offset提交给kafka内部topic："),a("strong",[s._v("__consumer_offsets")]),s._v("，提交过去的时候，"),a("strong",[s._v("key是consumerGroupId+topic+分区号，value就是当前offset的值")]),s._v("，kafka会定期清理topic里的消息，最后就保留最新的那条数据")]),s._v(" "),a("p",[s._v("因为__consumer_offsets可能会接收高并发的请求，kafka默认给其"),a("strong",[s._v("分配50个分区")]),s._v("(可以通过offsets.topic.num.partitions设置)，这样可以通过加机器的方式抗大并发。")]),s._v(" "),a("p",[s._v("通过如下公式可以选出consumer消费的offset要提交到__consumer_offsets的哪个分区")]),s._v(" "),a("p",[s._v("公式："),a("strong",[s._v("hash(consumerGroupId)  %  __consumer_offsets主题的分区数")])]),s._v(" "),a("p",[a("strong",[s._v("消费者Rebalance机制")])]),s._v(" "),a("p",[s._v("rebalance就是说如果消费组里的消费者数量有变化或消费的分区数有变化，kafka会重新分配消费者消费分区的关系。比如consumer group中某个消费者挂了，此时会自动把分配给他的分区交给其他的消费者，如果他又重启了，那么又会把一些分区重新交还给他。")]),s._v(" "),a("p",[s._v("**注意：**rebalance只针对subscribe这种不指定分区消费的情况，如果通过assign这种消费方式指定了分区，kafka不会进行rebanlance。")]),s._v(" "),a("p",[s._v("如下情况可能会触发消费者rebalance")]),s._v(" "),a("ol",[a("li",[s._v("消费组里的consumer增加或减少了")]),s._v(" "),a("li",[s._v("动态给topic增加了分区")]),s._v(" "),a("li",[s._v("消费组订阅了更多的topic")])]),s._v(" "),a("p",[s._v("rebalance过程中，消费者无法从kafka消费消息，这对kafka的TPS会有影响，如果kafka集群内节点较多，比如数百个，那重平衡可能会耗时极多，所以应尽量避免在系统高峰期的重平衡发生。")]),s._v(" "),a("p",[a("strong",[s._v("消费者Rebalance分区分配策略：")])]),s._v(" "),a("p",[s._v("主要有三种rebalance的策略：range、round-robin、sticky。")]),s._v(" "),a("p",[s._v("Kafka 提供了消费者客户端参数partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。"),a("strong",[s._v("默认情况为range分配策略")]),s._v("。")]),s._v(" "),a("p",[s._v("假设一个主题有10个分区(0-9)，现在有三个consumer消费：")]),s._v(" "),a("p",[a("strong",[s._v("range策略")]),s._v("就是按照分区序号排序，假设 n＝分区数／消费者数量 = 3， m＝分区数%消费者数量 = 1，那么前 m 个消费者每个分配 n+1 个分区，后面的（消费者数量－m ）个消费者每个分配 n 个分区。")]),s._v(" "),a("p",[s._v("比如分区0~3给一个consumer，分区4~6给一个consumer，分区7~9给一个consumer。")]),s._v(" "),a("p",[a("strong",[s._v("round-robin策略")]),s._v("就是轮询分配，比如分区0、3、6、9给一个consumer，分区1、4、7给一个consumer，分区2、5、8给一个consumer")]),s._v(" "),a("p",[a("strong",[s._v("sticky策略")]),s._v("初始时分配策略与round-robin类似，但是在rebalance的时候，需要保证如下两个原则。")]),s._v(" "),a("p",[s._v("1）分区的分配要尽可能均匀 。")]),s._v(" "),a("p",[s._v("2）分区的分配尽可能与上次分配的保持相同。")]),s._v(" "),a("p",[s._v("当两者发生冲突时，第一个目标优先于第二个目标 。这样可以最大程度维持原来的分区分配的策略。")]),s._v(" "),a("p",[s._v("比如对于第一种range情况的分配，如果第三个consumer挂了，那么重新用sticky策略分配的结果如下：")]),s._v(" "),a("p",[s._v("consumer1除了原有的0~3，会再分配一个7")]),s._v(" "),a("p",[s._v("consumer2除了原有的4~6，会再分配8和9")]),s._v(" "),a("p",[a("strong",[s._v("Rebalance过程如下")])]),s._v(" "),a("p",[s._v("当有消费者加入消费组时，消费者、消费组及组协调器之间会经历以下几个阶段。")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/51.png",alt:""}})]),s._v(" "),a("p",[a("strong",[s._v("第一阶段：选择组协调器")])]),s._v(" "),a("p",[a("strong",[s._v("组协调器GroupCoordinator")]),s._v("：每个consumer group都会选择一个broker作为自己的组协调器coordinator，负责监控这个消费组里的所有消费者的心跳，以及判断是否宕机，然后开启消费者rebalance。")]),s._v(" "),a("p",[s._v("consumer group中的每个consumer启动时会向kafka集群中的某个节点发送 FindCoordinatorRequest 请求来查找对应的组协调器GroupCoordinator，并跟其建立网络连接。")]),s._v(" "),a("p",[a("strong",[s._v("组协调器选择方式")]),s._v("：")]),s._v(" "),a("p",[s._v("consumer消费的offset要提交到__consumer_offsets的哪个分区，这个分区leader对应的broker就是这个consumer group的coordinator")]),s._v(" "),a("p",[a("strong",[s._v("第二阶段：加入消费组****JOIN GROUP")])]),s._v(" "),a("p",[s._v("在成功找到消费组所对应的 GroupCoordinator 之后就进入加入消费组的阶段，在此阶段的消费者会向 GroupCoordinator 发送 JoinGroupRequest 请求，并处理响应。然后GroupCoordinator 从一个consumer group中选择第一个加入group的consumer作为leader("),a("strong",[s._v("消费组协调器")]),s._v(")，把consumer group情况发送给这个leader，接着这个leader会负责制定分区方案。")]),s._v(" "),a("p",[a("strong",[s._v("第三阶段（ SYNC GROUP)")])]),s._v(" "),a("p",[s._v("consumer leader通过给GroupCoordinator发送SyncGroupRequest，接着GroupCoordinator就把分区方案下发给各个consumer，他们会根据指定分区的leader broker进行网络连接以及消息消费。")]),s._v(" "),a("p",[a("strong",[s._v("producer发布消息机制剖析")])]),s._v(" "),a("p",[a("strong",[s._v("1、写入方式")])]),s._v(" "),a("p",[s._v("producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。")]),s._v(" "),a("p",[a("strong",[s._v("2、消息路由")])]),s._v(" "),a("p",[s._v("producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：")]),s._v(" "),a("p",[s._v("\\1. 指定了 patition，则直接使用； 2. 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition 3. patition 和 key 都未指定，使用轮询选出一个 patition。")]),s._v(" "),a("p",[a("strong",[s._v("3、写入流程")])]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/52.png",alt:""}})]),s._v(" "),a("p",[s._v('\\1. producer 先从 zookeeper 的 "/brokers/.../state" 节点找到该 partition 的 leader 2. producer 将消息发送给该 leader 3. leader 将消息写入本地 log 4. followers 从 leader pull 消息，写入本地 log 后 向leader 发送 ACK 5. leader 收到'),a("strong",[s._v("所有 ISR 中")]),s._v("的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK")]),s._v(" "),a("p",[a("strong",[s._v("HW与LEO详解")])]),s._v(" "),a("p",[s._v("HW俗称高水位，HighWatermark的缩写，取一个partition对应的"),a("strong",[s._v("ISR中最小的LEO("),a("strong",[a("strong",[s._v("log-end-offset")])]),s._v(")作为HW")]),s._v("，consumer最多只能消费到HW所在的位置。另外每个replica都有HW,leader和follower各自负责更新自己的HW的状态。对于leader新写入的消息，consumer不能立刻消费，leader会等待该消息被所有ISR中的replicas同步后更新HW，此时消息才能被consumer消费。这样就保证了如果leader所在的broker失效，该消息仍然可以从新选举的leader中获取。对于来自内部broker的读取请求，没有HW的限制。")]),s._v(" "),a("p",[s._v("下图详细的说明了当producer生产消息至broker后，ISR以及HW和LEO的流转过程：")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/53.png",alt:""}})]),s._v(" "),a("p",[a("strong",[s._v("由此可见，Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的follower都复制完，这条消息才会被commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，follower异步的从leader复制数据，数据只要被leader写入log就被认为已经commit，这种情况下如果follower都还没有复制完，落后于leader时，突然leader宕机，则会丢失数据。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。再回顾下消息发送端对发出消息持久化机制参数"),a("strong",[a("strong",[s._v("acks")])]),s._v("的设置，我们结合HW和LEO来看下acks=1的情况")])]),s._v(" "),a("p",[a("strong",[s._v("结合HW和LEO看下 acks=1的情况")])]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/54.png",alt:""}})]),s._v(" "),a("p",[a("strong",[s._v("日志分段存储")])]),s._v(" "),a("p",[s._v("Kafka 一个分区的消息数据对应存储在一个文件夹下，以topic名称+分区号命名，消息在分区内是分段(segment)存储，每个段的消息都存储在不一样的log文件里，这种特性方便old segment file快速被删除，kafka规定了一个段位的 log 文件最大为 1G，做这个限制目的是为了方便把 log 文件加载到内存去操作：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# 部分消息的offset索引文件，kafka每次往分区发4K(可配置)消息就会记录一条当前消息的offset到index文件，\n# 如果要定位消息的offset会先在这个文件里快速定位，再去log文件里找具体消息\n00000000000000000000.index\n# 消息存储文件，主要存offset和消息体\n00000000000000000000.log\n# 消息的发送时间索引文件，kafka每次往分区发4K(可配置)消息就会记录一条当前消息的发送时间戳与对应的offset到timeindex文件，\n# 如果需要按照时间来定位消息的offset，会先在这个文件里查找\n00000000000000000000.timeindex\n\n00000000000005367851.index\n00000000000005367851.log\n00000000000005367851.timeindex\n\n00000000000009936472.index\n00000000000009936472.log\n00000000000009936472.timeindex\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("这个 9936472 之类的数字，就是代表了这个日志段文件里包含的起始 Offset，也就说明这个分区里至少都写入了接近 1000 万条数据了。")]),s._v(" "),a("p",[s._v("Kafka Broker 有一个参数，log.segment.bytes，限定了每个日志段文件的大小，最大就是 1GB。")]),s._v(" "),a("p",[s._v("一个日志段文件满了，就自动开一个新的日志段文件来写入，避免单个文件过大，影响文件的读写性能，这个过程叫做 log rolling，正在被写入的那个日志段文件，叫做 active log segment。")]),s._v(" "),a("p",[a("strong",[s._v("最后附一张zookeeper节点数据图：")])]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/55.png",alt:""}})]),s._v(" "),a("h2",{attrs:{id:"kafka生产环境问题总结及性能实践"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka生产环境问题总结及性能实践"}},[s._v("#")]),s._v(" kafka生产环境问题总结及性能实践")]),s._v(" "),a("h3",{attrs:{id:"kafka可视化管理工具kafka-manager"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kafka可视化管理工具kafka-manager"}},[s._v("#")]),s._v(" Kafka可视化管理工具kafka-manager")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/60.png",alt:""}})]),s._v(" "),a("p",[s._v("安装及基本使用可参考：https://www.cnblogs.com/dadonggg/p/8205302.html")]),s._v(" "),a("h3",{attrs:{id:"线上环境规划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#线上环境规划"}},[s._v("#")]),s._v(" "),a("strong",[s._v("线上环境规划")])]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/61.jpg",alt:""}})]),s._v(" "),a("h3",{attrs:{id:"jvm参数设置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#jvm参数设置"}},[s._v("#")]),s._v(" "),a("strong",[s._v("JVM参数设置")])]),s._v(" "),a("p",[s._v("kafka是scala语言开发，运行在JVM上，需要对JVM参数合理设置，参看JVM调优专题")]),s._v(" "),a("p",[s._v("修改bin/kafka-start-server.sh中的jvm设置，假设机器是32G内存，可以如下设置：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('export KAFKA_HEAP_OPTS="-Xmx16G -Xms16G -Xmn10G -XX:MetaspaceSize=256M -XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:G1HeapRegionSize=16M"\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("这种大内存的情况一般都要用G1垃圾收集器，因为年轻代内存比较大，用G1可以设置GC最大停顿时间，不至于一次minor gc就花费太长时间，当然，因为像kafka，rocketmq，es这些中间件，写数据到磁盘会用到操作系统的page cache，所以JVM内存不宜分配过大，需要给操作系统的缓存留出几个G。")]),s._v(" "),a("h3",{attrs:{id:"线上问题及优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#线上问题及优化"}},[s._v("#")]),s._v(" "),a("strong",[s._v("线上问题及优化")])]),s._v(" "),a("h4",{attrs:{id:"_1、消息丢失情况"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、消息丢失情况"}},[s._v("#")]),s._v(" 1、消息丢失情况：")]),s._v(" "),a("p",[a("strong",[s._v("消息发送端")]),s._v("：")]),s._v(" "),a("p",[s._v("（1）acks=0： 表示producer不需要等待任何broker确认收到消息的回复，就可以继续发送下一条消息。性能最高，但是最容易丢消息。大数据统计报表场景，对性能要求很高，对数据丢失不敏感的情况可以用这种。")]),s._v(" "),a("p",[s._v("（2）acks=1： 至少要等待leader已经成功将数据写入本地log，但是不需要等待所有follower是否成功写入。就可以继续发送下一条消息。这种情况下，如果follower没有成功备份数据，而此时leader又挂掉，则消息会丢失。")]),s._v(" "),a("p",[s._v("（3）acks=-1或all： 这意味着leader需要等待所有备份(min.insync.replicas配置的备份个数)都成功写入日志，这种策略会保证只要有一个备份存活就不会丢失数据。这是最强的数据保证。一般除非是金融级别，或跟钱打交道的场景才会使用这种配置。当然如果min.insync.replicas配置的是1则也可能丢消息，跟acks=1情况类似。")]),s._v(" "),a("p",[a("strong",[s._v("消息消费端")]),s._v("：")]),s._v(" "),a("p",[s._v("如果消费这边配置的是自动提交，万一消费到数据还没处理完，就自动提交offset了，但是此时你consumer直接宕机了，未处理完的数据丢失了，下次也消费不到了。")]),s._v(" "),a("h4",{attrs:{id:"_2、消息重复消费"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、消息重复消费"}},[s._v("#")]),s._v(" 2、消息重复消费")]),s._v(" "),a("p",[a("strong",[s._v("消息发送端")]),s._v("：")]),s._v(" "),a("p",[s._v("发送消息如果配置了重试机制，比如网络抖动时间过长导致发送端发送超时，实际broker可能已经接收到消息，但发送方会重新发送消息")]),s._v(" "),a("p",[a("strong",[s._v("消息消费端")]),s._v("：")]),s._v(" "),a("p",[s._v("如果消费这边配置的是自动提交，刚拉取了一批数据处理了一部分，但还没来得及提交，服务挂了，下次重启又会拉取相同的一批数据重复处理")]),s._v(" "),a("p",[s._v("一般消费端都是要做"),a("strong",[s._v("消费幂等")]),s._v("处理的。")]),s._v(" "),a("h4",{attrs:{id:"_3、消息乱序"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、消息乱序"}},[s._v("#")]),s._v(" 3、消息乱序")]),s._v(" "),a("p",[s._v("如果发送端配置了重试机制，kafka不会等之前那条消息完全发送成功才去发送下一条消息，这样可能会出现，发送了1，2，3条消息，第一条超时了，后面两条发送成功，再重试发送第1条消息，这时消息在broker端的顺序就是2，3，1了")]),s._v(" "),a("p",[s._v("所以，是否一定要配置重试要根据业务情况而定。也可以用同步发送的模式去发消息，当然acks不能设置为0，这样也能保证消息发送的有序。")]),s._v(" "),a("p",[s._v("kafka保证全链路消息顺序消费，需要从发送端开始，将所有有序消息发送到同一个分区，然后用一个消费者去消费，但是这种性能比较低，可以在消费者端接收到消息后将需要保证顺序消费的几条消费发到内存队列(可以搞多个)，一个内存队列开启一个线程顺序处理消息。")]),s._v(" "),a("h4",{attrs:{id:"_4、消息积压"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、消息积压"}},[s._v("#")]),s._v(" 4、消息积压")]),s._v(" "),a("p",[s._v("1）线上有时因为发送方发送消息速度过快，或者消费方处理消息过慢，可能会导致broker积压大量未消费消息。")]),s._v(" "),a("p",[s._v("此种情况如果积压了上百万未消费消息需要紧急处理，可以修改消费端程序，让其将收到的消息快速转发到其他topic(可以设置很多分区)，然后再启动多个消费者同时消费新主题的不同分区。")]),s._v(" "),a("p",[s._v("2）由于消息数据格式变动或消费者程序有bug，导致消费者一直消费不成功，也可能导致broker积压大量未消费消息。")]),s._v(" "),a("p",[s._v("此种情况可以将这些消费不成功的消息转发到其它队列里去(类似"),a("strong",[s._v("死信队列")]),s._v(")，后面再慢慢分析死信队列里的消息处理问题。")]),s._v(" "),a("h4",{attrs:{id:"_5、延时队列"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5、延时队列"}},[s._v("#")]),s._v(" 5、延时队列")]),s._v(" "),a("p",[s._v("延时队列存储的对象是延时消息。所谓的“延时消息”是指消息被发送以后，并不想让消费者立刻获取，而是等待特定的时间后，消费者才能获取这个消息进行消费，延时队列的使用场景有很多， 比如 ：")]),s._v(" "),a("p",[s._v("1）在订单系统中， 一个用户下单之后通常有 30 分钟的时间进行支付，如果 30 分钟之内没有支付成功，那么这个订单将进行异常处理，这时就可以使用延时队列来处理这些订单了。")]),s._v(" "),a("p",[s._v("2）订单完成1小时后通知用户进行评价。")]),s._v(" "),a("p",[s._v("**实现思路：**发送延时消息时先把消息按照不同的延迟时间段发送到指定的队列中（topic_1s，topic_5s，topic_10s，...topic_2h，这个一般不能支持任意时间段的延时），然后通过定时器进行轮训消费这些topic，查看消息是否到期，如果到期就把这个消息发送到具体业务处理的topic中，队列中消息越靠前的到期时间越早，具体来说就是定时器在一次消费过程中，对消息的发送时间做判断，看下是否延迟到对应时间了，如果到了就转发，如果还没到这一次定时任务就可以提前结束了。")]),s._v(" "),a("h4",{attrs:{id:"_6、消息回溯"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6、消息回溯"}},[s._v("#")]),s._v(" 6、消息回溯")]),s._v(" "),a("p",[s._v("如果某段时间对已消费消息计算的结果觉得有问题，可能是由于程序bug导致的计算错误，当程序bug修复后，这时可能需要对之前已消费的消息重新消费，可以指定从多久之前的消息回溯消费，这种可以用consumer的offsetsForTimes、seek等方法指定从某个offset偏移的消息开始消费，参见上节课的内容。")]),s._v(" "),a("h4",{attrs:{id:"_7、分区数越多吞吐量越高吗"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7、分区数越多吞吐量越高吗"}},[s._v("#")]),s._v(" 7、分区数越多吞吐量越高吗")]),s._v(" "),a("p",[s._v("可以用kafka压测工具自己测试分区数不同，各种情况下的吞吐量")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# 往test里发送一百万消息，每条设置1KB\n# throughput 用来进行限流控制，当设定的值小于 0 时不限流，当设定的值大于 0 时，当发送的吞吐量大于该值时就会被阻塞一段时间\nbin/kafka-producer-perf-test.sh --topic test --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=192.168.65.60:9092 acks=1\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/62.png",alt:""}})]),s._v(" "),a("p",[s._v("网络上很多资料都说分区数越多吞吐量越高 ， 但从压测结果来看，分区数到达某个值吞吐量反而开始下降，实际上很多事情都会有一个临界值，当超过这个临界值之后，很多原本符合既定逻辑的走向又会变得不同。一般情况分区数跟集群机器数量相当就差不多了。")]),s._v(" "),a("p",[s._v("当然吞吐量的数值和走势还会和磁盘、文件系统、 I/O调度策略等因素相关。")]),s._v(" "),a("p",[s._v('注意：如果分区数设置过大，比如设置10000，可能会设置不成功，后台会报错"java.io.IOException : Too many open files"。')]),s._v(" "),a("p",[s._v("异常中最关键的信息是“ Too many open flies”，这是一种常见的 Linux 系统错误，通常意味着文件描述符不足，它一般发生在创建线程、创建 Socket、打开文件这些场景下 。 在 Linux系统的默认设置下，这个文件描述符的个数不是很多 ，通过 ulimit -n 命令可以查看：一般默认是1024，可以将该值增大，比如：ulimit -n 65535")]),s._v(" "),a("h4",{attrs:{id:"_8、消息传递保障"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8、消息传递保障"}},[s._v("#")]),s._v(" 8、消息传递保障")]),s._v(" "),a("ul",[a("li",[s._v("at most once(消费者最多收到一次消息，0--1次)：acks = 0 可以实现。")]),s._v(" "),a("li",[s._v("at least once(消费者至少收到一次消息，1--多次)：ack = all 可以实现。")]),s._v(" "),a("li",[s._v("exactly once(消费者刚好收到一次消息)：at least once 加上消费者幂等性可以实现，还可以用"),a("strong",[s._v("kafka生产者的幂等性")]),s._v("来实现。")])]),s._v(" "),a("p",[a("strong",[s._v("kafka生产者的幂等性")]),s._v("：因为发送端重试导致的消息重复发送问题，kafka的幂等性可以保证重复发送的消息只接收一次，只需在生产者加上参数 props.put(“enable.idempotence”, true) 即可，默认是false不开启。")]),s._v(" "),a("p",[s._v("具体实现原理是，kafka每次发送消息会生成PID和Sequence Number，并将这两个属性一起发送给broker，broker会将PID和Sequence Number跟消息绑定一起存起来，下次如果生产者重发相同消息，broker会检查PID和Sequence Number，如果相同不会再接收。")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("PID：每个新的 Producer 在初始化的时候会被分配一个唯一的 PID，这个PID 对用户完全是透明的。生产者如果重启则会生成新的PID。\nSequence Number：对于每个 PID，该 Producer 发送到每个 Partition 的数据都有对应的序列号，这些序列号是从0开始单调递增的。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("h4",{attrs:{id:"_9、kafka的事务"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9、kafka的事务"}},[s._v("#")]),s._v(" 9、kafka的事务")]),s._v(" "),a("p",[s._v("Kafka的事务不同于Rocketmq，Rocketmq是保障本地事务(比如数据库)与mq消息发送的事务一致性，Kafka的事务主要是保障一次发送多条消息的事务一致性(要么同时成功要么同时失败)，一般在kafka的流式计算场景用得多一点，比如，kafka需要对一个topic里的消息做不同的流式计算处理，处理完分别发到不同的topic里，这些topic分别被不同的下游系统消费(比如hbase，redis，es等)，这种我们肯定希望系统发送到多个topic的数据保持事务一致性。Kafka要实现类似Rocketmq的分布式事务需要额外开发功能。")]),s._v(" "),a("p",[s._v("kafka的事务处理可以参考"),a("a",{attrs:{href:"http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("官方文档"),a("OutboundLink")],1),s._v("：")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v(' Properties props = new Properties();\n props.put("bootstrap.servers", "localhost:9092");\n props.put("transactional.id", "my-transactional-id");\n Producer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\n //初始化事务\n producer.initTransactions();\n\n try {\n     //开启事务\n     producer.beginTransaction();\n     for (int i = 0; i < 100; i++){\n         //发到不同的主题的不同分区\n         producer.send(new ProducerRecord<>("hdfs-topic", Integer.toString(i), Integer.toString(i)));\n         producer.send(new ProducerRecord<>("es-topic", Integer.toString(i), Integer.toString(i)));\n         producer.send(new ProducerRecord<>("redis-topic", Integer.toString(i), Integer.toString(i)));\n     }\n     //提交事务\n     producer.commitTransaction();\n } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {\n     // We can\'t recover from these exceptions, so our only option is to close the producer and exit.\n     producer.close();\n } catch (KafkaException e) {\n     // For all other exceptions, just abort the transaction and try again.\n     //回滚事务\n     producer.abortTransaction();\n }\n producer.close();\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br")])]),a("p",[s._v("​")]),s._v(" "),a("p",[a("strong",[s._v("10、kafka高性能的原因")])]),s._v(" "),a("ul",[a("li",[s._v("磁盘顺序读写：kafka消息不能修改以及不会从文件中间删除保证了磁盘顺序读，kafka的消息写入文件都是追加在文件末尾，不会写入文件中的某个位置(随机写)保证了磁盘顺序写。")]),s._v(" "),a("li",[s._v("数据传输的零拷贝")]),s._v(" "),a("li",[s._v("读写数据的批量batch处理以及压缩传输")])]),s._v(" "),a("p",[s._v("数据传输零拷贝原理：")]),s._v(" "),a("p",[s._v("​    "),a("img",{attrs:{src:"https://gitee.com/nylg/picture/raw/master/kafka/63.png",alt:""}})]),s._v(" "),a("p",[s._v("​")])])}),[],!1,null,null,null);a.default=r.exports}}]);